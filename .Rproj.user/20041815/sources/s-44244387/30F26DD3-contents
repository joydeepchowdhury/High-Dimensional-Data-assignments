--- 
title: "Assignments: Inference for High Dimensional Data"
author: "Joydeep Chowdhury"
# date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [references.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "Assignment problems for the course Inference for High Dimensional Data in M.Stat 2nd year, 2019-2020, at Indian Statistical Institute, Kolkata."
---

# Introduction {-}

This document contains the assignment problems and the materials needed to solve them for the course _Inference for High Dimensional Data_ in M.Stat 2^nd^ year, 2019--2020, at Indian Statistical Institute, Kolkata.

```{r include = FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  'glmnet', 'gglasso', 'genlasso'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

# Regularized regression {#regularizations}

Consider a linear regression problem, where $Y$ is the $n \times 1$ vector of real valued responses, $X$ is an $n \times p$ matrix of corresponding covariate values, $\beta$ is the $p \times 1$ vector of coefficients, and we model $E[Y] = X \beta$.
When $p > n$, or, in general when $X'X$ is singular or nearly singular (e.g., in the presence of [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)), the usual least squares method of estimating $\beta$ fails to work satisfactorily. Further, for large $p$, i.e., in the presence of a large number of covariates in the model, one may be interested in finding whether only a few covariates disproportionately affect the response. One may be also interested in imposing a degree of smoothness among the consecutive elements in the coefficient vector $\beta$. In such situations, we apply regularization in the form of a penalty term based on $\beta$.

Among many different regularization methods, we shall describe the following and demonstrate their computations in simulated data:

- Lasso
- Ridge regression
- Elastic net
- Adaptive lasso
- Group lasso
- Fused lasso

## Lasso and other regularizations {#regularizations-description}

In this section, different types of regularizations and their utilities are described.

### Lasso {#lasso-description}

In the lasso setup, we minimize the loss function
\begin{align*}
L_{lasso}(\beta) = \frac{1}{2n} \| Y - X \beta \|^2_2 + \lambda \| \beta \|_1
\end{align*}
with respect to $\beta$. Here, $\lambda \ge 0$ is a tuning parameter. For $\lambda = 0$, the lasso formulation reduces to the usual least squares setup. For large enough $\lambda$, the minimizer $\beta$ is $0$.

The utility of the lasso formulation in variable selection stems from the fact that $q = 1$ is the smallest value for which the $l_q$ norm is convex. The variable selection property of $l_q$ penalized methods improves with decreasing $q$. On the other hand, it is relatively far easier computationally to solve a convex optimization problem than a non-convex one. For example, the $l_0$ penalization is the purest form of variable selection method. However, the minimization of a $l_0$ penalized criterion is computationally very challenging.

The lasso (**l**east **a**bsolute **s**hrinkage and **s**election **o**perator) method was proposed by @tibshirani1996regression.

During numerical computations, the tuning parameter $\lambda$ is selected using cross validation.

Usually, before computing the lasso estimate, the response as well as the covariates are centered so that average of $Y$ and the averages of the columns of $X$ are all equal to $0$. This justifies the omission of the intercept term in the linear regression model (i.e., omission of $\beta_0$ in the model $Y = \beta_0 1 + X \beta$). It is easy to recover the coefficeints for the uncentered case from those for the centerd case. In case the covariates are recorded in different units, additionally we may also standardize them so that $X_{\cdot i}' X_{\cdot i} = 1$ for all $j$, where $X_{\cdot i}$ is the $i$th column of $X$. This makes the model free of the effect of the units of the covariates.

### Ridge regression {#ridge-description}

In the ridge regression setup, the loss function is
\begin{align*}
L_{ridge}(\beta) = \frac{1}{2n} \| Y - X \beta \|^2_2 + \frac{\lambda}{2} \| \beta \|_2^2 .
\end{align*}
Here again, $\lambda \ge 0$ is a tuning parameter.

Unlike the lasso, the ridge regression exhibits no variable selection property. It shrinks all the coefficients together rather than reducing some of them to 0. The ridge regression method is also known as the Tikhonov regularization, named after Andrey Tikhonov [@tikhonov1943stability]. The ridge regression method works well in the presence of multicollinearity: when some covariate variables are highly correlated.

The tuning parameter $\lambda$ is selected via cross validation while computing the estimate of $\beta$.

### Elastic net {#elasticnet-description}

it is observed that the lasso method does not work well when some covariate variables are highly correlated. However, in this particular area, ridge regression excels. Based on this observation, the elastic net method is developed combining the penalization terms of the lasso and the ridge regression, and the loss function here is
\begin{align*}
L_{elnet}(\beta) = \frac{1}{2n} \| Y - X \beta \|^2_2 + \lambda \left[ ((1 - \alpha) / 2) \| \beta \|_2^2 + \alpha \| \beta \|_1 \right] .
\end{align*}
Here, $0 \le \alpha \le 1$ balances the charactersitics of the lasso and the ridge regression in the elastic net formulation, and $\lambda \ge 0$ is a tuning parameter. For $\alpha = 0$, the ealstic net formulation coincides with the ridge regression method, and $\alpha = 1$ makes the eastic net method identical to the lasso.

The elastic net method was first proposed by @zou2005regularization. What is behind the curious name _elastic net_? In the words of its authors,

> Similar to the lasso, the elastic net simultaneously does automatic variable selection and continuous shrinkage, and it can select groups of correlated variables. It is like a stretchable fishing net that retains 'all the big fish'.
> --- @zou2005regularization.^[There is also a story behind the name _lasso_, involving an execution device and a gentle Canadian. Look up @tibshirani2011regression if you are interested in the history of the lasso!]

For moderate values of $\alpha$, say, $\alpha = 0.5$, the elastic net method tends to either select groups of highly correlated covariates together or discards them together.

A problem that the lasso method suffers is that when $p > n$, it can select at most $n$ non-zero coefficients. This limitation may be undesirable in some high dimensional setup. The elastic net for $\alpha < 1$ does not suffer from this issue.

While computing the estimate of $\beta$ from the sample using the elastic net method, one may choose both the parameters $\alpha$ and $\lambda$ through cross validation. Or, one may fix the elastic net parameter $\alpha$ at some appropriate value, and choose $\lambda$ through cross validation.

### Adaptive lasso {#adaptivelasso-description}

Sometimes, the lasso method is not found to be efficient enough for variable selection, and it ends up selecting too many variables. The adaptive lasso method, proposed by @zou2006adaptive, is useful in such situations. The adaptive lasso is a two-step method, which is started with an initial estimate $\tilde{\beta}$. Then, the loss function becomes
\begin{align*}
L_{adaptive}(\beta) = \frac{1}{2n} \| Y - X \beta \|^2_2 + \lambda \sum_{i=1}^p w_i | \beta_i | ,
\end{align*}
where $w_i = \left| \tilde{\beta}_i \right|^{-\gamma}$ for some $\gamma > 0$, $i = 1, \ldots, p$, and $\lambda \ge 0$ is a tuning parameter. Note that given $\tilde{\beta}$, $L_{adaptive}(\beta)$ is convex in $\beta$. In fact, using a scale transformation of $\beta$ and the covariates, $L_{adaptive}(\beta)$ can be expressed in the form of the lasso loss function $L_{lasso}(\beta)$, and hence can be solved using the algorithms developed for the lasso method. The parameter $\gamma$ is often taken to be 1 for the sake of simplicity [@buhlmann2011statistics].

Taking $r / 0 = \infty$ for $r > 0$, it follows that whenever $\lambda > 0$, for any $\tilde{\beta}_i = 0$, we must have $\hat{\beta}_i = 0$, where $\hat{\beta}$ is the adaptive lasso estimate. Therefore, the solution of the adaptive lasso problem is sparser than the initial estimate. Further, if for any particular $i$, $\tilde{\beta}_i$ is large, then the adaptive lasso estimate for $\beta_i$ experiences less shrinkage due to a low value of the weight $w_i$ and corresponding low penalty. This implies low bias for the estimate of that particular $\beta_i$.

There are many ways to choose the initial estimate $\tilde{\beta}$. When $n > p$, one can take the ordinary least squares estimate as $\tilde{\beta}$ [@zou2006adaptive]. When $p \ge n$, for $i = 1, \ldots, p$, one can take $\tilde{\beta}_i$ to be the ordinary least squares estimate for the univariate regression problem with $Y$ as the response and $X_{\cdot i}$, the $i$th column of $X$, as the covariate and no intercept term. Particularly for high dimensional models or in case of variable selection problems, it is beneficial to take the lasso estimate with a cross-validated tuning parameter as $\tilde{\beta}$, because the two-step setup of the adaptive lasso formulation then yields a sparser solution than the usual lasso estimate [@buhlmann2011statistics].

The sparsness inducing property of the adaptive lasso can be amplified by extending it to a multi-step method instead of a two-step method. Here, at each step, the weights are computed based on the estimate obtained at the last step [@buhlmann2008discussion].

Here again, the tuning parameter $\lambda$ of the adaptive lasso can be selected via cross validation while computing the estimate.

The basic idea of the adaptive lasso is of introducing weights to individual coefficients, based on which we can control the level of shrinking of those coefficients in our adaptive lasso estimate. Different customizations of the usual lasso or elastic net formulations are possible based on this concept of weight introduction.

### Group lasso {#grouplasso-description}

In some situations, it is required that we either select or drop a group of covariate variables together in our model. For example, if a covariate variable is categorical in nature, its levels are represented using dummy variables. Consider the following problem for illustration. Suppose we have one categorical covariate, $Z$, with $K$ levels $z_1, \ldots, z_K$, and $p$ numeric covariates, which we represent using the covariate matrix $X$. Then, our regression model is $Y = \sum_{i=1}^K \alpha_i U_i + X \beta$, where $U_i = 1$ if $Z = z_i$ and $0$ otherwise. Naturally, we would like to either include all of the variables $U_1, \ldots, U_K$ together in the model or drop them together. But the usual lasso has no such provision.

This problem is addressed by the group lasso method proposed by @yuan2006model. Suppose the covariate variables are partitioned in $G$ groups $X_1, \ldots, X_G$. Then, the group lasso loss function is
\begin{align*}
L_{group}(\beta_1, \ldots, \beta_G) = \frac{1}{2} \left\| Y - \sum_{g=1}^G X_g \beta_g \right\|^2_2 + \lambda \sum_{g=1}^G w_g \| \beta_g \|_2 .
\end{align*}
Here, $w_g$'s are weights assigned to the groups. One may take $w_g = 1$ for all $g$, which assigns equal weights to all the groups. However, one may also want to make larger groups less likely to be selected than a smaller group. The way to do that would be to pick a weighing scheme which assigns higher weights  to larger groups. In particular, the authors @yuan2006model recommended to take $w_g = s_g$, where $s_g$ is the size of the $g$th group, $g = 1, \ldots, G$, to address this problem. We shall take $w_g = 1$ for all $g$ for the sake of simplicity. As before, $\lambda \ge 0$ is a tuning parameter.

We minimize $L_{group}(\beta_1, \ldots, \beta_G)$ with respect to $\beta_1, \ldots, \beta_G$ to get the group lasso estimate. When all the group sizes are 1, the group lasso method coincides with the usual lasso method.

One drawback of the ususal group lasso is that it does not induce sparsity within the groups, i.e., if a group is selected in the model, all the coefficients within the group are usually non-zero. The casue of this is the $l_2$ norm within the groups, similar to the ridge regression. This problem can be mitigated using the underlying idea of the elastic net, and modifying the group lasso loss function to
\begin{align*}
L_{modgroup}(\beta_1, \ldots, \beta_G) = \frac{1}{2} \left\| Y - \sum_{g=1}^G X_g \beta_g \right\|^2_2 + \lambda \sum_{g=1}^G [ (1 - \alpha) \| \beta_g \|_2 + \alpha \| \beta_g \|_1 ] ,
\end{align*}
where $0 \le \alpha \le 1$. When $\alpha = 1$, the modified loss function $L_{modgroup}(\beta_1, \ldots, \beta_G)$ reduces to the usual lasso loss function, yielding the usual lasso estimate.

The tuning parameter $\lambda$ is selected based on cross validation. One may choose $\alpha$ either through cross validation, or may fix it beforehand.

### Fused lasso {#fusedlasso-description}

Sometimes, there is a spatial or temporal structure behind the data, and we want a degree of smoothness among the adjacent coefficents. Suppose the coefficients $\beta_i$ correspond to some quantity recorded on a one dimensional grid, and we do not want $\beta_{i+1}$ and $\beta_{i-1}$ to be much different from $\beta_i$. To achieve this goal, we may take the loss function as
\begin{align*}
L_{fused}(\beta) = \frac{1}{2} \| Y - X \beta \|^2_2 + \lambda_1 \| \beta \|_1 + \lambda_2 \sum_{i=2}^p | \beta_i - \beta_{i-1} | .
\end{align*}
Here, $\lambda_1 \ge 0, \lambda_2 \ge 0$ are tuning parameters. For positive $\lambda_2$, higher differences among consecutive coefficients are penalized higher. For spatial models, the coefficients may correspond to entities on a two or higher dimensional grid, and the concept of _adjacent_ coefficients is meaningful. To impose a degree of smoothness among the adjacent coefficients, we may use the loss function
\begin{align*}
L_{fusedgrid}(\beta) = \frac{1}{2} \| Y - X \beta \|^2_2 + \lambda_1 \| \beta \|_1 + \lambda_2 \sum_{i, j \in A} | \beta_i - \beta_j | ,
\end{align*}
where $A$ is the set of all pairs of adjacent indices. This way of penalization imposes a degree of _fusion_ among the adjacent coefficients (and hence the name fused lasso).

In some cases, we may not even have a regression problem, but the idea behind the fused lasso can still be applied. For example, suppose the response values are recorded on a one dimensional spatial or temporal grid, and we want to model the mean of the response variable. The mean is here a function over the grid, and we do not want much variation over adjacent grid points. To achieve that, we may take the loss function as
\begin{align*}
L_{simplefused}(\beta) = \frac{1}{2} \| Y - \beta \|^2_2 + \lambda_1 \| \beta \|_1 + \lambda_2 \sum_{i=2}^p | \beta_i - \beta_{i-1} | .
\end{align*}

The methodology of fused lasso was proposed by @tibshirani2005sparsity.

We have two tuning parameters here. Using careful arguments, some procedures for computing the fused lasso estimate reduce the problem of selecting two tuning parameters to the problem of selecting just one. Then, that one tuning parameter may be selected via cross validation. Other procedures also exist.

### Other regularizations {#otherregularizations-description}

Above, we have described several types of regularizations based on variations of the penalty term in the loss function. Through combinations of the underlying ideas behind them, many other forms of regularizations are possible. However, we have also change the first term of the loss function, which is fixed to be a squared error loss in all the formulations above. Observe that the squared error loss can be interpreted as the negative of the log likelihood of a Gaussian distribution (the associated constants would not matter in the minimization problem). So, based on this idea, we can readily construct other loss functions based on the negatives of the log likelihoods of other distributions.

We can also formulate such regularizations for generalized linear models, where, rather than modeling $E[Y]$ as a linear function of the covariate $X$, we model $g(E[Y]) = X \beta$.

In any such formulations, as long as the loss function is convex in $\beta$, solving the the minimization problem is not hard. However, some non-convex formulations also exist, but for general non-convex problems, getting the solution is computationally hard, and actually reaching the global minimum may not be guaranteed.

## Numerical demonstration {#regularizations-demonstration}

We shall use the `glmnet` package in R for the computations related to lasso, ridge regression, elastic net and adaptive lasso. The `glmnet` package is developed by @R-glmnet.

Let $\Sigma$ be a $100 \times 100$ covariance matrix defined by $\sigma_{i j} = 0.5 + 0.5 \mathbb{I}(i = j)$. Let $X$ be a $100$ dimensional random vector with $X \sim N_{100}( 0, \Sigma )$. Denote the $i$th coordinate of $X$ as $X_i$, $i = 1, \ldots, 100$. We consider the following regression problem: $Y = (1 - X_1 + 2 X_2 - 3 X_3 + 4 X_4 - 5 X_5 + 6 X_6) + e$, where $e$ is an error variable independent of $X$. We have a sample of size $30$ on $Y$ and $X$, based on which we shall estimate the coefficient vector using different types of regularizations.

```{r}

# Data generation; setting the seed for reproducible outcome
set.seed(1234)
Mu = rep(0, 100)
SIGMA = matrix(0.5, nrow = 100, ncol = 100) +
  diag(0.5, nrow = 100, ncol = 100)
X = MASS::mvrnorm(n = 30, mu = Mu, Sigma = SIGMA)
X_with_intercept = cbind(1, X)
Beta = c(c(1, -1, 2, -3, 4, -5, 6), rep(0, (100 - 6)))
Y = X_with_intercept %*% Beta + rnorm(30)

```

First we need to install the package `glmnet`.

```{r eval = FALSE}

install.packages('glmnet')

```

### Lasso computation {#lasso-computation}

We now demonstrate the lasso method on this model.

```{r message = FALSE}

# Loading the 'glmnet' package
require(glmnet)

# Constructing the lasso solution paths
fit_lasso = glmnet(X, Y)

```

We can plot the lasso solution paths using the `plot` function. The indices of the covariate variables are written on the left side. Notice that the coefficient paths for $X_6$ and $X_5$ are the most prominent, and then comes $X_4$. Do you understand the underlying cause?

```{r}

plot(fit_lasso, xvar = 'lambda', label = TRUE)

```

Using the function `cv.glmnet`, we can find the value of the cross-validated tuning parameter $\lambda$.

```{r}

cv_fit_lasso = cv.glmnet(X, Y)

```

We can visually inspect the mean-squared error for different values of lambda; the two bars along the mean-squared error curve are the upper and the lower standard deviation curves.

```{r}

plot(cv_fit_lasso)

```

In the plot above, we note that two specific $\lambda$ values are marked by vertical dotted lines. One of them, denoted by `lambda.min`, is the value of $\lambda$ which corresponds to the minimum cross-validated error. The other one, denoted by `lambda.1se`, is the largest value of $\lambda$ such that the corresponding cross-validated error is within one standard error of the minimum. This value of $\lambda$ corresponds to the _most regularized model_ with the cross-validated error being within one standard error of the minimum cross-validated error. In case of the lasso, since with increasing $\lambda$ the number of non-zero coefficients decreases, the number of non-zero coefficients corresponding to `lambda.1se` is typically lower than that corresponding to `lambda.min`.
 
We can access the lambda value corresponding to the minimum cross-validated error in the following way.

```{r}

lambda_min_lasso = cv_fit_lasso$lambda.min
writeLines(paste('Cross-validated lambda for lasso:',
                 lambda_min_lasso))

```

We can get the coefficients of the fitted model at the cross-validated lambda in the following way.

```{r}

coefficients_lasso = coef(cv_fit_lasso, s = 'lambda.min')
print(coefficients_lasso)

```

As can be seen above, the coefficients are returned in a sparse matrix format. We can convert that to the usual numeric format in the following way. Remember that the intercept term will occupy the first position in the resulting vector.

```{r}

coefficients_lasso = as.numeric(coefficients_lasso)

# Finding the number of zero coefficients
number_zero_coefficients_lasso = sum(coefficients_lasso == 0)
writeLines(paste('Number of zero coefficients in lasso for',
                 'cross-validated lambda:\n',
                 number_zero_coefficients_lasso))

# Finding which covariates have a non-zero coefficient in the
# fitted model; the '-1' accounts for the intercept term
indices_nonzero_coefficients_lasso =
  which(coefficients_lasso != 0) - 1

```

To get the coefficients at the value of $\lambda$ which gives the most regularized model such that the cross-validated error is within one standard error of the minimum, we use the following command.

```{r}

coefficients_lasso_most_regularized =
  coef(cv_fit_lasso, s = 'lambda.1se')
coefficients_lasso_most_regularized =
  as.numeric(coefficients_lasso_most_regularized)

# Finding the number of zero coefficients
number_zero_coefficients_lasso_most_regularized =
  sum(coefficients_lasso_most_regularized == 0)
writeLines(paste('Number of zero coefficients in lasso for \n',
                 'the most regularized model:',
                 number_zero_coefficients_lasso_most_regularized))

```

Note that the number of zero coefficients corresponding to `lambda.1se` is higher than that corresponding to `lambda.min`. We can find the coefficients at some other value of lambda also, in the same way.

```{r}

coefficients_lasso_s = coef(cv_fit_lasso, s = 0.5)
print(coefficients_lasso_s)

```

To predict the response at some value of the covariates, we use the following command. The argument `newx` must be supplied as a matrix and not a vector.

```{r}

x_0 = matrix(rnorm(100), nrow = 1, ncol = 100)
predicted_Y = predict(cv_fit_lasso, newx = x_0, s = 'lambda.min')
writeLines(paste('Predicted response:', predicted_Y))

```

We can predict at several sets of values of the covariates using the same command.

```{r}

x_0_values = matrix(rnorm(3*100), nrow = 3, ncol = 100)
predict(cv_fit_lasso, newx = x_0_values, s = 'lambda.min')

```

We can also predict at different values of lambda.

```{r}

x_0_values = matrix(rnorm(3*100), nrow = 3, ncol = 100)
predict(cv_fit_lasso, newx = x_0_values, s = c(0.5, 0.8))

```

### Ridge regression computation {#ridge-computation}

We next demonstrate the ridge regression method. The commands for the ridge regression method are identical with the lasso, except passing the value of another parameter `alpha = 0` to the `glmnet` function, which overwrites the default value of `alpha = 1`. The default value `alpha = 1` corresponds to the lasso method, and the value `alpha = 0` corresponds to ridge regression method. 

```{r}

# Constructing the ridge regression solution paths; 'alpha = 0'
# corresponds to the ridge regression method
fit_ridge = glmnet(X, Y, alpha = 0)

# Plotting the ridge regression solution paths
plot(fit_ridge, xvar = 'lambda', label = TRUE)

```

Note that the coefficients do not become zero with inreasing $\lambda$. The function 'cv.glmnet' with `alpha = 0` yields the value of the cross-validated $\lambda$ for the ridge regression.

```{r}

# 'alpha = 0' corresponds to the ridge regression
cv_fit_ridge = cv.glmnet(X, Y, alpha = 0)

# Plotting the mean-squared error for different values of lambda
plot(cv_fit_ridge)

# Accessing the lambda value corresponding to the minimum
# cross-validated error
lambda_min_ridge = cv_fit_ridge$lambda.min
writeLines(paste('Cross-validated lambda for ridge:',
                 lambda_min_ridge))

```

Below we obtain the coefficients of the fitted ridge regression model at the cross-validated lambda.

```{r}

coefficients_ridge = coef(cv_fit_ridge, s = 'lambda.min')
print(coefficients_ridge)

```

Note that the coefficients are nonzero unlike the lasso. In case of the ridge regression, the value `lambda.1se` is not so interesting as in case of the lasso, as here there is no question of reducing any coefficient to zero.

```{r}

coefficients_ridge = as.numeric(coefficients_ridge)

# Finding the number of nonzero coefficients; the '-1' accounts
# for the intercept term
number_nonzero_coefficients_ridge =
  sum(coefficients_ridge != 0) - 1
writeLines(paste('Number of nonzero coefficients for',
                 'the ridge regression:\n',
                 number_nonzero_coefficients_ridge))

```

Finding the coefficients at some other value of lambda is the same as in the case of the lasso.

```{r}

coefficients_ridge_s = coef(cv_fit_ridge, s = 0.5)
print(coefficients_ridge_s)

```

We can also predict at several sets of values of the covariates and at different values of lambda for the fitted ridge regression model. The argument `newx` always must be a matrix, even if we are predicting at only one set of values of the covariates.

```{r}

x_0_values = matrix(rnorm(3*100), nrow = 3, ncol = 100)
predict(cv_fit_ridge, newx = x_0_values, s = c(0.5, 0.8))

```

### Elastic net computation {#elasticnet-computation}

From the description of the elastic net in subsection \@ref(elasticnet-description), and the codes for computing the lasso and the ridge regression solutions in subsection \@ref(lasso-computation) and subsection \@ref(ridge-computation) respectively, you might have guessed that the function `glmnet` actually computed the elastic net solutions for different values of the parameter $\alpha$: $\alpha = 1$ corresponds to the lasso and $\alpha = 0$ corresponds to the ridge regression method. So, the same code with a value of the argument `alpha` in $(0, 1)$ will return the elastic net solution for that value of $\alpha$. Below, we demonstrate the computation for $\alpha = 0.5$.

```{r}

# Constructing the elastic net solution paths for 'alpha = 0.5'
fit_elnet = glmnet(X, Y, alpha = 0.5)

# Plotting the elastic net solution paths
plot(fit_elnet, xvar = 'lambda', label = TRUE)

```

Note that the coefficients become zero with inreasing $\lambda$ like in the case of the lasso.

```{r}

# Cross-validation for elastic net with 'alpha = 0.5'
cv_fit_elnet = cv.glmnet(X, Y, alpha = 0.5)

# Plotting the mean-squared error for different values of lambda
plot(cv_fit_elnet)

# Accessing the lambda value corresponding to the minimum
# cross-validated error
lambda_min_elnet = cv_fit_elnet$lambda.min
writeLines(paste('Cross-validated lambda for elastic net:',
                 lambda_min_elnet))

# Coefficients of the fitted elastic net at the cross-validated
# lambda
coefficients_elnet = coef(cv_fit_elnet, s = 'lambda.min')
print(coefficients_elnet)

```

Note that many of the coefficients are zero like the lasso. However, the number of zero coefficients is lower than the lasso.

```{r}

coefficients_elnet = as.numeric(coefficients_elnet)

# Finding the number of nonzero coefficients; the '-1' accounts
# for the intercept term
number_nonzero_coefficients_elnet =
  sum(coefficients_elnet != 0) - 1
writeLines(paste('Number of nonzero coefficients for',
                 'the elastic net:',
                 number_nonzero_coefficients_elnet))

# Computing the coefficients at some other value of lambda
coefficients_elnet_s = coef(cv_fit_elnet, s = 0.5)
print(coefficients_elnet_s)

# Predicting at several sets of values of the covariates and at
# different values of lambda for the fitted elastic net
x_0_values = matrix(rnorm(3*100), nrow = 3, ncol = 100)
predict(cv_fit_elnet, newx = x_0_values, s = c(0.5, 0.8))

```

### Adaptive lasso computation {#adaptivelasso-computation}

Recall the description of the adaptive lasso method in subsection \@ref(adaptivelasso-description). We shall demonstrate the adaptive lasso with $\gamma = 1$ taking the usual cross-validated lasso solution as the initial estimate.

First we compute the cross-validated lasso solution.

```{r}

# Computing the coefficient for the cross-validated lasso estimate
cv_fit_lasso = cv.glmnet(X, Y)
coefficients_lasso = coef(cv_fit_lasso, s = 'lambda.min')

# Dropping the intercept term in the cross-validated lasso
# estimate
coefficients_lasso = as.numeric(coefficients_lasso)[-1]

# Computing the adaptive lasso estimate. The argument
# 'penalty.factor' assigns weights to the coefficients, with the
# convention of r / 0 = Infinity for r > 0.
cv_fit_adaptive =
  cv.glmnet(X, Y, penalty.factor = 1 / abs(coefficients_lasso))
coefficients_adaptive = coef(cv_fit_adaptive, s = 'lambda.min')

# Dropping the intercept term for the adaptive lasso estimate
coefficients_adaptive = as.numeric(coefficients_adaptive)[-1]

```

Recall from subsection \@ref(adaptivelasso-description) that the adaptive lasso estimate is sparser than the lasso estimate. From the construction of our simulation setup, we know that only the coefficients of $X_1$, $X_2$, $X_3$, $X_4$, $X_5$ and $X_6$ are nonzero in the underlying model. Let us see the indices of the covariates with nonzero coefficients in the lasso estimate and the adaptive lasso estimate.

```{r}

writeLines(paste('Indices of the covariates with nonzero',
                 'coefficients \n for the lasso estimate:\n',
                 paste(which(coefficients_lasso != 0),
                       collapse = ' ')))

writeLines(paste('Indices of the covariates with nonzero',
                 'coefficients \n for the adaptive lasso',
                 'estimate:\n',
                 paste(which(coefficients_adaptive != 0),
                       collapse = ' ')))

```

We note that the lasso estimate has `r sum(coefficients_lasso != 0)` nonzero coordinates, while the adaptive lasso has `r sum(coefficients_adaptive != 0)` nonzero coordinates. So, the adaptive lasso estimate is indeed sparser than the lasso estimate. However, both of them erroneously estimate several zero coefficients as nonzero, and fail to capture several nonzero coefficients.

### Group lasso computation {#grouplasso-computation}

For the group lasso computation, we shall use the `gglasso` package by @R-gglasso. We shall use the same simulation setup, but the covariates will be divided in groups in the following way: $X_1, \ldots, X_5$ in the first group, $X_6, \ldots, X_10$ in the second group, and so on. We form the group indices below, which will be required.

```{r}

group_indices = c()
for (i in 1:20)
  group_indices = c(group_indices, rep(i, 5))
writeLines('First 20 group indices are:')
print(group_indices[1:20])

```

The following command installs the package `gglasso`.

```{r eval = FALSE}

install.packages('gglasso')

```

We now demonstrate the computation for the group lasso method. The commands are very similar to those in `glmnet`.

```{r message = FALSE}

# Loading the 'gglasso' package
require(gglasso)

# Constructing the group lasso solution paths
fit_group = gglasso(X, Y, group = group_indices)

# Plotting the ridge regression solution paths; the
# 'label' argument does not work here, unfortunately.
plot(fit_group, xvar = 'lambda')

# Cross-validation for group lasso; 'nfolds' is a
# cross-validation parameter whose default value
# was 10 in our earlier calculations using the 'glmnet'
# package. Since its default parameter for the 'gglasso'
# package is different, we set that explicitly to 10
# here to maintain uniformity.
cv_fit_group = cv.gglasso(X, Y, group = group_indices,
                          nfolds = 10)

# Plotting the mean-squared error for different values of
# lambda in the group lasso cross-validation
plot(cv_fit_group)

# Getting the lambda value corresponding to the minimum
# cross-validated error in the group lasso
lambda_min_group = cv_fit_group$lambda.min
writeLines(paste('Cross-validated lambda for group lasso:',
                 lambda_min_group))

# Coefficients of the fitted group lasso at the
# cross-validated lambda
coefficients_group = coef(cv_fit_group, s = 'lambda.min')
print(coefficients_group)

# Computing the coefficients at some other value of lambda
# for the group lasso estimate
coefficients_group_s = coef(cv_fit_group, s = 0.5)
print(coefficients_group_s)

```

Notice that the group lasso method reduces to zero all the coefficients together in any group. Next, we predict the group lasso estimate at some sets of values of the covariates.

```{r}

# Predicting at several sets of values of the covariates and at
# different values of lambda for the fitted group lasso; here
# 'type' is an argument whose value 'link' corresponds to
# regression prediction.
x_0_values = matrix(rnorm(3*100), nrow = 3, ncol = 100)
predict(cv_fit_group, newx = x_0_values, s = c(0.5, 0.8),
        type = 'link')

```

### Fused lasso computation {#fusedlasso-computation}

We shall demonstrate the idea of the fused lasso in a simpler model unlike the simulated regression setup used in the previous cases. Assume the response values are $Y_i$, where $i = 1, \ldots, n$, and $Y_i \sim \theta_i + e_i$, where $e_i$ are independent and identically distributed $N(0, 0.25)$ random variables. Suppose the sequence $\{\theta_i\}$ is piecewise constant, such that $\theta_1 = \ldots = \theta_{20} = 2$, $\theta_{21} = \ldots = \theta_{30} = 3$, $\theta_{31} = \ldots = \theta_{40} = 0$, $\theta_{41} = \ldots = \theta_{60} = 5$, $\theta_{61} = \ldots = \theta_{85} = 1$ and $\theta_{85} = \ldots = \theta_{100} = 0$. We want to find
$$\hat{\theta} = \arg\min_{\theta \in \mathbb{R}^n} \left[ \sum_{i=1}^n (Y_i - \theta_i)^2 + \lambda \sum_{i=1}^{n-1} | \theta_i - \theta_{i+1} | \right].$$
Below, we generate a sample from this model with $n = 100$.

```{r}

# Data generation for fused lasso with fixed seed
set.seed(1234)
Theta = c(rep(2, 20), rep(3, 10), rep(0, 10), rep(5, 20),
          rep(1, 25), rep(0, 15))
Y = Theta + rnorm(100, mean = 0, sd = sqrt(0.25))

```

We need the `genlasso` package [@R-genlasso] for the fused lasso computation. The following command would install this package.

```{r eval = FALSE}

install.packages('genlasso')

```

The fused lasso computation is demonstrated below.

```{r message = FALSE}

# Loading the 'genlasso' package
require(genlasso)

# Constructing the fused lasso solution paths; since
# the coefficients here are recorded on a one dimensional
# grid, we use the function 'fusedlasso1d'.
fit_fused = fusedlasso1d(Y)

# Plotting the fused lasso solution paths along with the
# actual observations; the points are the actual observations.
plot(fit_fused)

```

We note that the plot here is very different from the earlier examples, due to the difference in the nature of the problems (and also the packages used).

```{r echo = TRUE, results = 'hide'}

# Cross-validation for fused lasso; 'k' is  the
# cross-validation parameter corresponding to 'nfolds' in
# earlier cases. Cross-validation is done using the
# 'cv.trendfilter' function.
cv_fit_fused = cv.trendfilter(fit_fused, k = 10)

```

```{r}

# Plotting the mean-squared error for different values of
# lambda in the fused lasso cross-validation
plot(cv_fit_fused)

# Getting the lambda value corresponding to the minimum
# cross-validated error in the fused lasso
lambda_min_fused = cv_fit_fused$lambda.min
writeLines(paste('Cross-validated lambda for fused lasso:',
                 lambda_min_fused))

# Plotting the fused lasso estimate for the
# cross-validated lambda along with the actual observations
plot(fit_fused, lambda = lambda_min_fused,
     main = paste('Estimated coefficients for the',
                  'cross-validated lambda'))

```

<!--chapter:end:01-lasso.Rmd-->

# Multiple testing {#multipletesting}

In many situations, we face with the problem of conducting a collection of statistical tests simultaneously based on the same experiment. Consider the following problems:

1. A researcher is investigating which genes make one individual susceptible to a particular disease, say, a cancer. To investigate that, data on [gene expression levels](https://en.wikipedia.org/wiki/Gene_expression#Measurement) are collected over 10000 genes from 100 normal individuals and 100 patients. The culprit genes making one susceptible to the particular disease can be discovered by finding the genes whose expression level differ significantly between a normal individual and a patient. Consequently, the researcher faces a problem of conducting 10000 statistical tests simultaneously based on a sample of 200. Such problems are common in [genetic association](https://en.wikipedia.org/wiki/Genetic_association) studies.

2. A pharmaceutical company considers a new drug for the treatment of a particular disease, and proceeds to investigate its efficacy. Tearment of a disease often consists of alleviating of its symptoms, which can be many. So, the company is faced with conducting a [randomized controlled trial](https://en.wikipedia.org/wiki/Randomized_controlled_trial) on patients, administering on group placebo and the other the experimental drug, and then conducting statistical tests to identify significant differences between the experimental group undergoing the treatment and the control group for each of the plethora of symptoms.

In genetic studies as described in the first example, subseaquent investigation is carried out for each of the genes discovered to have a significant effect. Often, the actual number of genes associated with a disease does not exceed a few dozens. If that is the case here, and still one carries out usual hypothesis testing at 5\% level for each of the 10000 genes, it is expected that the researcher will get nearly $(10000 \times 0.05) = 500$ _false positives_, which are genes that are incorrectly identified as having a significant association with the disease. This is far higher than the number of actual genes influencing the susceptibility to the disease, which means significant resources lost in further investigation in all these false positive genes.

In the second example, suppose the new drug is no better than the placebo or no intervention. However, if there are 20-30 symptoms and usual statistical tests are carried out for each of these at 5\% level, it is very probable that the new drug is found to have a significant effect for at least one syndrom.

The above examples demonstrate the limitation of usual methods of hypothesis testing while considering a family of hypotheses simultaneously for an experiment. We state the problem formally below.

Suppose we want to test $m$ hypotheses $H_1, \ldots, H_m$ simultaneously for an experiment, and $m_0$ of these are actually true. Let $R$ be the number of hypotheses rejected. The following table describes the different variables on which we shall concentrate further.

Table: (\#tab:multipletesting) Classification of outcomes in multiple testing

---------------------------------------------------------------------------------------------
                                  Null hypothesis is true   Null hypothesis is false   Total
-------------------------------- ------------------------- -------------------------- -------
Fails to reject null hypothesis         $U$                        $W$                $m - R$

Rejects null hypothesis                 $V$                        $S$                $R$

Total                                   $m_0$                      $m - m_0$          $m$
---------------------------------------------------------------------------------------------

Of the elements in the above table, $R$ is the only observable variable. $R$ is the total number of _discoveries_. $S$ is the number of _true positives_ (also called true discoveries), while $V$ is the number of _false positives_ (also called false discoveries). In situations like the one described in the second example, one would like to bound the probability of $V \ge 1$ (because, for example, rejection of even one hypothesis means accepting the new drug as better than placebo). The quantity $P[ V \ge 1 ]$ is called the _familywise error rate_ (**FWER**).

## Controlling the familywise error rate {#FWER}

Since $P[ V \ge 1 ]$ is the probability of rejecting at least one true null hypothesis, by ensuring $P[ V \ge 1 ] \le \alpha$, we bound the maximum familywise error rate by $\alpha$. Below, several methods to achieve this are described.

### Bonferroni correction {#bonferroni}

Suppose the level of each of the $m$ hypotheses $H_1, \ldots, H_m$ is $\beta$. From the [Boole's inequality](https://en.wikipedia.org/wiki/Boole%27s_inequality), we have $P[ V \ge 1 ] = P\left[ \cup_{i=1}^{m} \{H_i \text{ is rejected while being true}\} \right] \le \sum_{i=1}^{m} P[H_i \text{ is rejected while being true}] = m \beta$. Therefore, to ensure FWER $\le \alpha$, we take the level of each of the individual hypotheses as $\alpha / m$.

The Bonferroni correction is very conservative in nature (has a high type II error), as the level of each of the individual tests is very low for even moderate $p$. Also, when the test statistics for the hypotheses are not independent, which is often the case for high dimensional setup, the actual FWER corresponding to the Bonferroni correction is considerably lower than $\alpha$. This can be understood from the fact that $P[A \cup B] = P[A] + P[B] - P[A \cap B]$, which implies that for $A$, $B$ being dependent events, $P[A \cup B] < P[A] + P[B]$.

Several methods were developed subsequently to mitigate the conservative nature of the Bonferroni correction. These are based on the principle of rejecting the hypotheses with the lowest p-values only, and accepting the rest.

### Holm's method {#holm}

The Holm's method, developed by @holm1979simple, first orders the p-values $P_1, \ldots, P_m$ corresponding to the $m$ hypotheses $H_1, \ldots, H_m$ in the increasing order. Let the ordered p-values be $P_{(1)}, \ldots, P_{(m)}$, and the corresponding hypotheses be $H_{(1)}, \ldots, H_{(m)}$. For a given level $\alpha$, let $k$ be the smallest integer such that $$P_{(k)} > \frac{\alpha}{m - k + 1}.$$ The Holm's method rejects the hypotheses $H_{(1)}, \ldots, H_{(k-1)}$, and does not reject $H_{(k)}, \ldots, H_{(m)}$. If $k = 1$, it does not reject any null hypotheses, and if no such $k$ exists, it rejects all the null hypotheses.

It is easy to see that, whenever the Bonferroni correction rejects a particular hypothesis, the Holm's method also rejects it, but not the contrary. It can be proved that FWER $\le \alpha$ for the Holm's method. Hence, the Holm's method is uniformly more powerful than the Bonferroni correction.

### Hochberg's method {#hochberg}

Hochberg's method [@hochberg1988sharper] also orders the p-values in the increasing order as in \@ref(holm). However, it finds the largest integer $k$ such that $$P_{(k)} \le \frac{\alpha}{m - k + 1},$$ and then rejects $H_{(1)}, \ldots, H_{(k)}$ but does not reject $H_{(k+1)}, \ldots, H_{(m)}$.

At the first glance, the Holm's method and the Hochberg's method may seem identical, but in fact Hochberg's method is uniformly more powerful than the former. This is due to the fact that the Hochberg's method rejects $H_{(1)}, \ldots, H_{(k)}$ whenever the Holm's method rejects them, but not the other way. Intuitively, because the Hochberg's method considers the p-values from the highest to the lowest, it is always inclined to reject more hypotheses than the Holm's method.
However, unlike the Holm's method, whose bound on FWER does not need any assumption on the underlying distribution or dependence among the p-values or test statistics, the Hochberg's method does (@sarkar1997simes, @sarkar1998some). Its bound on FWER is valid in particular when the test statistics for the individual hupotheses are indepdendent or positively correlated.

### Hommel's method {#hommel}

In the Hommel's method [@hommel1988stagewise], one again orders the p-values as in \@ref(holm), then finds the maximum integer $j$ in $1, \ldots, n$ such that $$P_{(n - j + k)} > \frac{k \alpha}{j}$$ for all $k = 1, \ldots, j$. If such a maximum $j$ does not exist, one rejects all the null hypotheses $H_i$, $i = 1, \ldots, m$. Otherwise, one rejects the hypotheses whose corresponding p-values satisfy $P_i \le \alpha / j$.

It can be shown that the Hommel's method is more powerful than the Hochberg's method. However, the validity of the bound on its FWER also requires certain assumptions.

## False discovery rate and its control {#FDR}

In certain situations, it may not be appropriate to put a bound on the FWER. Consider the example of the gene expression experiment described in the beginning of Chapter \@ref(multipletesting). If there are, say, 30 genes with actual effect on the disease, and in the testing 2 false positives occur, then it is not a very significant problem in terms of drawing wrong conclusions and wastage of resources. Similarly, if there are 200 genes affecting the disease in reality, and we have 20 false positives, then again it is not a big issue. In such cases, controlling the FWER makes the statistical investigation too conservative for the present purpose, and a certain kind of adaptablity in the procedure is desired based on the number of true positives. In such cases, we want to control the number of false positives compared to the true positives, as opposed to the occurance of any false positives.

Consider the quantities described in Table \@ref(tab:multipletesting). Recall that the variable $R$ denotes the number of rejected null hypotheses (discoveries) and $V$ is the number of rejected true hypotheses (false discoveries). We want to put a bound on the expected value of the ratio $(V / R)$. The quantity $(V / R)$ is called the _false discovery proportion_, and $E[ V / R ]$ is called the _false discovery rate_ (**FDR**). We assume the convention of defining $0 / 0 = 0$. Below, two methods are described which control the FDR.

### Benjamini-Hochberg method {#BH}

The Benjamini-Hochberg method [@benjamini1995controlling] finds the largest integer $k$ such that $$P_{(k)} \le \frac{\alpha}{m} k$$ for a given level $\alpha$. It then rejects all $H_{(i)}$ for $i = 1, \ldots, k$.

This method can be represented by a plot of the ordered p-values along with a line passing through the origin having slope $(\alpha / m)$. The hypotheses, whose corresponding p-values are below the line, are rejected.

It can be shown that this method has a FDR less than or equal to \alpha when the test statistics are independent, and under certain depdendence conditions. However, this is not valid for arbitrary dependence structures.

### Benjamini–Yekutieli method {#BY}

The Benjamini–Yekutieli method [@benjamini2001control] modifies the Benjamini-Hochberg method so that it controls the FDR under general dependene structures. This method finds the largest $k$ such that $$P_{(k)} \le \frac{\alpha}{m h(m)} k ,$$ where $h(m) = \sum_{i=1}^m \frac{1}{i}$, and then rejects all $H_{(i)}$ for $i = 1, \ldots, k$. However, if the test statistics are independent or the dependence conditions in the Benjamini-Hochberg method are satisfied, one can take $h(m) = 1$.

The FDR of the Benjamini–Yekutieli method is bounded above by $\alpha$, as indicated earlier.

## Adjusted p-values {#adjusted-pvalues}

In hypothesis testing, we are often more interested in the p-value of the test instead of accepting or rejecting the null hypothesis at some specified level. Reporting the p-value only makes our inference independent of the level of the test. In multiple testing, we have seen that the methods are designed to reject or not reject the individual hypotheses based on their p-values and a pre-specified level. Nevertheless, for each of the methods described above, one can modify the p-values obtained for the individual hypothesis so that one can follow the usual procedure of hypothesis testing based on those modified p-values and any level and still control the FWER or FDR as intended. These modified p-values are called _adjusted p-values_. The adjusted p-values are numbers lying in the interval [0, 1], but they should not be interpreted as some probabilities like actual p-values.

- In the Bonferroni correction (\@ref(bonferroni)), one rejects hypothesis $H_i$ at level $\alpha$ if the corresponding p-value $P_i$ satisfies $P_i \le \alpha / m$. Therefore the adjusted p-value $\hat{P}_i$ for hypothesis $H_i$ for the Bonferroni correction is $\hat{P}_i = \min\{ m P_i, 1 \}$, $i = 1, \ldots, m$. The minimum with 1 is taken to bound the p-value in the interval $[0, 1]$.

- The adjusted p-values for the Holm's method (\@ref(holm)) are given by $\hat{P}_{(i)} = \min\left\{ \max\{ (m + 1 - j) P_{(j)} \,|\, j = 1, \ldots, i \}, 1 \right\}$, $i = 1, \ldots, m$. (Multiplication by $(m + 1 - j)$ is obvious. Why the cumulative maximum is taken? Convince yourself that it is needed.)

- The adjusted p-values for the Hochberg's method (\@ref(hochberg)) are given by $\hat{P}_{(i)} = \min\left\{ \min\{ (m + 1 - j) P_{(j)} \,|\, j = i, \ldots, m \}, 1 \right\}$, $i = 1, \ldots, m$. Note that, though the difference in the descriptions of the Holm's method and the Hochberg's method may not be obvious initially, the formulae for their adjusted p-values are clearly distinct.

- Calculation of the adjusted p-values for the Hommel's method (\@ref(hommel)) is relatively complicated. These are computed algorithmically (given by @wright1992adjusted), unlike the straightforward formulae for the other methods.

- The adjusted p-values for the Benjamini-Hochberg method (\@ref(BH)) are given by $\hat{P}_{(i)} = \min\left\{ \min\left\{ \frac{m}{j} P_{(j)} \,\middle|\, j = i, \ldots, m \right\}, 1 \right\}$, $i = 1, \ldots, m$.

- The adjusted p-values for the Benjamini–Yekutieli method (\@ref(BY)) are given by $\hat{P}_{(i)} = \min\left\{ \min\left\{ \frac{m h(m)}{j} P_{(j)} \,\middle|\, j = i, \ldots, m \right\}, 1 \right\}$, $i = 1, \ldots, m$, where $h(m) = \sum_{j=1}^m \frac{1}{j}$.

## Summary {#multipletesting-summary}

While testing simultaneously many hypotheses, one may be interested in controlling the probability of rejecting atleast one true null hypothesis (FWER) or the expected proportion of rejected true null hypotheses (FDR). There are several methods to achieve either one. The Bonferroni correction, the Holm's method, the Hochberg's method and the Hommel's method control the FWER. The Benjamini-Hochberg method and the Benjamini–Yekutieli method control the FDR.

All the methods described in this chapter make very broad assumptions about the dependence among the test statistics for the individual tests (or does not make any assumption about dependence; for example: the Bonferroni correction). As a consequence, these are relatively conservative procedures, and it is possible to develop methods which are more powerful than them by taking into account the dependence structure of the test statistics in the particular multiple testing scenario. Resampling based methods (bootstrap and permutation) are develop with this aim, and they generally are more powerful than the methods described before. Those are out of scope of the present document.

## Numerical demonstration {#multipletesting-demonstration}

All the methods for adjusted p-value computation described in \@ref(adjusted-pvalues) are implemented in the function `p.adjust` available in the R package **stats** (included with base R distribution). Below, the use of this function is demonstrated.

Consider a simple case where we have 10 hypotheses each having a p-value 0.04. What will be the adjusted p-values for the various methods described in sections \@ref(FWER) and \@ref(FDR)? Their computation is presented below.

```{r, results = 'hold'}

# Vector of p-values with length 10 and each element being 0.04
p = rep(0.04, 10)
writeLines(paste('Actual p-values:\n', paste(p, collapse = ' ')))

# Bonferroni correction
p_bonferroni = p.adjust(p, method = 'bonferroni')
writeLines(paste('Adjusted p-values for the',
                 'Bonferroni correction:\n',
                 paste(p_bonferroni, collapse = ' ')))

# Holm's method
p_holm = p.adjust(p, method = 'holm')
writeLines(paste('Adjusted p-values for the',
                 'Holm\'s method:\n',
                 paste(p_holm, collapse = ' ')))

# Hochberg's method
p_hochberg = p.adjust(p, method = 'hochberg')
writeLines(paste('Adjusted p-values for the',
                 'Hochberg\'s method:\n',
                 paste(p_hochberg, collapse = ' ')))

# Hommel's method
p_hommel = p.adjust(p, method = 'hommel')
writeLines(paste('Adjusted p-values for the',
                 'Hommel\'s method:\n',
                 paste(p_hommel, collapse = ' ')))

# Benjamini-Hochberg method
p_BH = p.adjust(p, method = 'BH')
writeLines(paste('Adjusted p-values for the',
                 'Benjamini-Hochberg method:\n',
                 paste(p_BH, collapse = ' ')))

# Benjamini–Yekutieli method
p_BY = p.adjust(p, method = 'BY')
writeLines(paste('Adjusted p-values for the',
                 'Benjamini–Yekutieli method:\n',
                 paste(round(p_BY, 3), collapse = ' ')))

```

From the output above, the difference between the adjusted p-values of the Holm's method and the Hochberg's method is noticeable. It corroborates our earlier assertion in subsection \@ref(hochberg) that the Hochberg's method is uniformly more powerful than the Holm's method, as every adjusted p-value for the Hochberg's method is smaller than the corresponding adjusted p-value for the Holm's method. However, though the Holm's method is uniformly more powerfull than the Bonferroni correction and the Hommel method is more powerful than the Hochberg's method, this toy example cannot confirm them, as all the corresponding adjusted p-values are identical. If we conduct simultaneous testing at level 0.05, the Bonferroni correction, the Holm's method and the Benjamini–Yekutieli method fail to reject any hothesis, whereas the Hochberg's method, the Hommel's method and the Benjamini-Hochberg method reject all the hypotheses.

Next, we consider a real dataset: the `golub` dataset from the R package **multtest** available in [Bioconductor](https://www.bioconductor.org/packages/release/bioc/html/multtest.html). This dataset contains the expression levels of 3051 genes based on 38 tumor mRNA samples from a leukemia microarray study presented in @golub1999molecular. Among the 38 tumor samples, 27 are acute lymphoblastic leukemia (ALL) cases and 11 are acute myeloid leukemia (AML) cases. The `golub` datatset is a matrix with 3051 rows and 38 columns. Each row corresponds to a gene, the first 27 columns correspond to ALL cases and the last 11 columns correspond to AML cases. We are interested in investigating the presence of statistically significant differences in expression levels for the 3051 genes among the two types of cancers.

First we need to install the 'multtest' package from Bioconductor, which can be done using the function 'install' from the package 'BiocManager'. The code in the next line would do this.

```{r eval = FALSE}

BiocManager::install('multtest')

```

Now, the computation procedure is demonstrated below.

```{r, results = 'hold'}

# Loading the data from the package 'multtest'
data('golub', package = 'multtest')

# Storing the data corresponding to ALL cases in the
# variable golub_ALL
golub_ALL = golub[,1:27]

# Storing the data corresponding to AML cases in the
# variable golub_AML
golub_AML = golub[,28:38]

# m is the number of tests, which is equal to the
# number of genes
m = nrow(golub)

# Computing the p-values using a two-sample t test
p = c()
for (i in 1:m)
  p[i] = t.test(golub_ALL[i,], golub_AML[i,],
                alternative = 'two.sided')$p.value

# Calculating the adjusted p-values
p_bonferroni = p.adjust(p, method = 'bonferroni')
p_holm = p.adjust(p, method = 'holm')
p_hochberg = p.adjust(p, method = 'hochberg')
p_hommel = p.adjust(p, method = 'hommel')
p_BH = p.adjust(p, method = 'BH')
p_BY = p.adjust(p, method = 'BY')

# Calculating the number of rejections based on level 0.05.
alpha = 0.05
writeLines(paste('Number of rejections based on',
                 'unadjusted p-values:', sum(p <= alpha)))
writeLines(paste('Number of rejections based on',
                 'Bonferroni correction:',
                 sum(p_bonferroni <= alpha)))
writeLines(paste('Number of rejections based on',
                 'Holm\'s method:',
                 sum(p_holm <= alpha)))
writeLines(paste('Number of rejections based on',
                 'Hochberg\'s method:',
                 sum(p_hochberg <= alpha)))
writeLines(paste('Number of rejections based on',
                 'Hommel\'s method:',
                 sum(p_hommel <= alpha)))
writeLines(paste('Number of rejections based on',
                 'Benjamini-Hochberg method:',
                 sum(p_BH <= alpha)))
writeLines(paste('Number of rejections based on',
                 'Benjamini–Yekutieli method:',
                 sum(p_BY <= alpha)))

```

We note that the Bonferroni's correction is the most conservative procedure, as expected. However, here the Holm's method and the Hochberg's method are also as much conservative as the Bonferroni's correction. The FDR controlling procedures are relatively less conservative compared to the FWER controlling procedures, again as expected.

<!--chapter:end:02-multipletesting.Rmd-->

# Assignments

1. Fix your roll number as the seed. Let $\Sigma$ be a $80 \times 80$ covariance matrix defined by $\sigma_{i j} = 0.6 + 0.6 \mathbb{I}(i = j)$. Let $X$ be a $80$ dimensional random vector with $X \sim N_{80}( 0, \Sigma )$. Denote the $i$th coordinate of $X$ as $X_i$, $i = 1, \ldots, 80$. Consider the following regression problem: $Y = (1 + X_1 + 2 X_2 - 3 X_3 - 4 X_4 + 5 X_5) + e$, where $e$ is an error variable independent of $X$. Generate a sample of size $30$ on $Y$ and $X$. Generate also a sample of size 5 from the distribution of $X$, and name the data matrix as $X_0$. Based on this sample,

    - Compute and plot the lasso solution paths.
    
    - Compute and visually present the outputs of the cross-validation process for the tuning parameter $\lambda$.
    
    - Print the value of $\lambda$ giving the minimum cross-validated error.
    
    - Print the value of $\lambda$ giving the most regularized model such that its cross-validated error is within one standard error of the minimum.
    
    - Print the estimated coefficients of the fitted lasso model for the value $\lambda$ being the one giving the minimum cross-validated error.
    
    - Print the estimated coefficients of the fitted lasso model for $\lambda = 0.5$.
    
    - Print the number of nonzero coefficients for the value $\lambda$ being the one giving the minimum cross-validated error.
    
    - Predict the response values at the covariate values in $X_0$ for $\lambda$ being the minimizer of the cross-validated error.

2. Based on the same data as above, solve all the above problems for the ridge regression method.

3. Again, based on the same data as above, solve all the above problems for the elastic net method, taking $\alpha = 0.5$.

4. Generate an independent sample of size $1000$ from the $Beta(1, 100)$ distribution, and assume that they are the p-values of $1000$ hypotheses tested simultaneously.

    - Compute the corresponding adjusted p-values for the Bonferroni correction, the Holm's method, the Hochberg's method, the Hommel's method, the Benjamini-Hochberg method and the Benjamini–Yekutieli method.
    
    - Compute the number of rejections in each case for the level being $0.05$.


Marks: (1): 10, (2): 3, (3): 3, (4): 4

<!--chapter:end:03-assignments.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:06-references.Rmd-->

