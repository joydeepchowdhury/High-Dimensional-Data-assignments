# Regularized regression {#regularizations}

Consider a linear regression problem, where $Y$ is the $n \times 1$ vector of real valued responses, $X$ is an $n \times p$ matrix of corresponding covariate values, $\beta$ is the $p \times 1$ vector of coefficients, and we model $E[Y] = X \beta$.
When $p > n$, or, in general when $X'X$ is singular or nearly singular (e.g., in the presence of [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)), the usual least squares method of estimating $\beta$ fails to work satisfactorily. Further, for large $p$, i.e., in the presence of a large number of covariates in the model, one may be interested in finding whether only a few covariates disproportionately affect the response. One may be also interested in imposing a degree of smoothness among the consecutive elements in the coefficient vector $\beta$. In such situations, we apply regularization in the form of a penalty term based on $\beta$.

Among many different regularization methods, we shall describe the following and demonstrate their computations in simulated data:

- Lasso
- Ridge regression
- Elastic net
- Adaptive lasso
- Group lasso
- Fused lasso

## Lasso and other regularizations {#regularizations-description}

In this section, different types of regularizations and their utilities are described.

### Lasso {#lasso-description}

In the lasso setup, we minimize the loss function
\begin{align*}
L_{lasso}(\beta) = \frac{1}{2n} \| Y - X \beta \|^2_2 + \lambda \| \beta \|_1
\end{align*}
with respect to $\beta$. Here, $\lambda \ge 0$ is a tuning parameter. For $\lambda = 0$, the lasso formulation reduces to the usual least squares setup. For large enough $\lambda$, the minimizer $\beta$ is $0$.

The utility of the lasso formulation in variable selection stems from the fact that $q = 1$ is the smallest value for which the $l_q$ norm is convex. The variable selection property of $l_q$ penalized methods improves with decreasing $q$. On the other hand, it is relatively far easier computationally to solve a convex optimization problem than a non-convex one. For example, the $l_0$ penalization is the purest form of variable selection method. However, the minimization of a $l_0$ penalized criterion is computationally very challenging.

The lasso (**l**east **a**bsolute **s**hrinkage and **s**election **o**perator) method was proposed by @tibshirani1996regression.

During numerical computations, the tuning parameter $\lambda$ is selected using cross validation.

Usually, before computing the lasso estimate, the response as well as the covariates are centered so that average of $Y$ and the averages of the columns of $X$ are all equal to $0$. This justifies the omission of the intercept term in the linear regression model (i.e., omission of $\beta_0$ in the model $Y = \beta_0 1 + X \beta$). It is easy to recover the coefficeints for the uncentered case from those for the centerd case. In case the covariates are recorded in different units, additionally we may also standardize them so that $X_{\cdot i}' X_{\cdot i} = 1$ for all $j$, where $X_{\cdot i}$ is the $i$th column of $X$. This makes the model free of the effect of the units of the covariates.

### Ridge regression {#ridge-description}

In the ridge regression setup, the loss function is
\begin{align*}
L_{ridge}(\beta) = \frac{1}{2n} \| Y - X \beta \|^2_2 + \frac{\lambda}{2} \| \beta \|_2^2 .
\end{align*}
Here again, $\lambda \ge 0$ is a tuning parameter.

Unlike the lasso, the ridge regression exhibits no variable selection property. It shrinks all the coefficients together rather than reducing some of them to 0. The ridge regression method is also known as the Tikhonov regularization, named after Andrey Tikhonov [@tikhonov1943stability]. The ridge regression method works well in the presence of multicollinearity: when some covariate variables are highly correlated.

The tuning parameter $\lambda$ is selected via cross validation while computing the estimate of $\beta$.

### Elastic net {#elasticnet-description}

it is observed that the lasso method does not work well when some covariate variables are highly correlated. However, in this particular area, ridge regression excels. Based on this observation, the elastic net method is developed combining the penalization terms of the lasso and the ridge regression, and the loss function here is
\begin{align*}
L_{elnet}(\beta) = \frac{1}{2n} \| Y - X \beta \|^2_2 + \lambda \left[ ((1 - \alpha) / 2) \| \beta \|_2^2 + \alpha \| \beta \|_1 \right] .
\end{align*}
Here, $0 \le \alpha \le 1$ balances the charactersitics of the lasso and the ridge regression in the elastic net formulation, and $\lambda \ge 0$ is a tuning parameter. For $\alpha = 0$, the ealstic net formulation coincides with the ridge regression method, and $\alpha = 1$ makes the eastic net method identical to the lasso.

The elastic net method was first proposed by @zou2005regularization. What is behind the curious name _elastic net_? In the words of its authors,

> Similar to the lasso, the elastic net simultaneously does automatic variable selection and continuous shrinkage, and it can select groups of correlated variables. It is like a stretchable fishing net that retains 'all the big fish'.
> --- @zou2005regularization.^[There is also a story behind the name _lasso_, involving an execution device and a gentle Canadian. Look up @tibshirani2011regression if you are interested in the history of the lasso!]

For moderate values of $\alpha$, say, $\alpha = 0.5$, the elastic net method tends to either select groups of highly correlated covariates together or discards them together.

A problem that the lasso method suffers is that when $p > n$, it can select at most $n$ non-zero coefficients. This limitation may be undesirable in some high dimensional setup. The elastic net for $\alpha < 1$ does not suffer from this issue.

While computing the estimate of $\beta$ from the sample using the elastic net method, one may choose both the parameters $\alpha$ and $\lambda$ through cross validation. Or, one may fix the elastic net parameter $\alpha$ at some appropriate value, and choose $\lambda$ through cross validation.

### Adaptive lasso {#adaptivelasso-description}

Sometimes, the lasso method is not found to be efficient enough for variable selection, and it ends up selecting too many variables. The adaptive lasso method, proposed by @zou2006adaptive, is useful in such situations. The adaptive lasso is a two-step method, which is started with an initial estimate $\tilde{\beta}$. Then, the loss function becomes
\begin{align*}
L_{adaptive}(\beta) = \frac{1}{2n} \| Y - X \beta \|^2_2 + \lambda \sum_{i=1}^p w_i | \beta_i | ,
\end{align*}
where $w_i = \left| \tilde{\beta}_i \right|^{-\gamma}$ for some $\gamma > 0$, $i = 1, \ldots, p$, and $\lambda \ge 0$ is a tuning parameter. Note that given $\tilde{\beta}$, $L_{adaptive}(\beta)$ is convex in $\beta$. In fact, using a scale transformation of $\beta$ and the covariates, $L_{adaptive}(\beta)$ can be expressed in the form of the lasso loss function $L_{lasso}(\beta)$, and hence can be solved using the algorithms developed for the lasso method. The parameter $\gamma$ is often taken to be 1 for the sake of simplicity [@buhlmann2011statistics].

Taking $r / 0 = \infty$ for $r > 0$, it follows that whenever $\lambda > 0$, for any $\tilde{\beta}_i = 0$, we must have $\hat{\beta}_i = 0$, where $\hat{\beta}$ is the adaptive lasso estimate. Therefore, the solution of the adaptive lasso problem is sparser than the initial estimate. Further, if for any particular $i$, $\tilde{\beta}_i$ is large, then the adaptive lasso estimate for $\beta_i$ experiences less shrinkage due to a low value of the weight $w_i$ and corresponding low penalty. This implies low bias for the estimate of that particular $\beta_i$.

There are many ways to choose the initial estimate $\tilde{\beta}$. When $n > p$, one can take the ordinary least squares estimate as $\tilde{\beta}$ [@zou2006adaptive]. When $p \ge n$, for $i = 1, \ldots, p$, one can take $\tilde{\beta}_i$ to be the ordinary least squares estimate for the univariate regression problem with $Y$ as the response and $X_{\cdot i}$, the $i$th column of $X$, as the covariate and no intercept term. Particularly for high dimensional models or in case of variable selection problems, it is beneficial to take the lasso estimate with a cross-validated tuning parameter as $\tilde{\beta}$, because the two-step setup of the adaptive lasso formulation then yields a sparser solution than the usual lasso estimate [@buhlmann2011statistics].

The sparsness inducing property of the adaptive lasso can be amplified by extending it to a multi-step method instead of a two-step method. Here, at each step, the weights are computed based on the estimate obtained at the last step [@buhlmann2008discussion].

Here again, the tuning parameter $\lambda$ of the adaptive lasso can be selected via cross validation while computing the estimate.

The basic idea of the adaptive lasso is of introducing weights to individual coefficients, based on which we can control the level of shrinking of those coefficients in our adaptive lasso estimate. Different customizations of the usual lasso or elastic net formulations are possible based on this concept of weight introduction.

### Group lasso {#grouplasso-description}

In some situations, it is required that we either select or drop a group of covariate variables together in our model. For example, if a covariate variable is categorical in nature, its levels are represented using dummy variables. Consider the following problem for illustration. Suppose we have one categorical covariate, $Z$, with $K$ levels $z_1, \ldots, z_K$, and $p$ numeric covariates, which we represent using the covariate matrix $X$. Then, our regression model is $Y = \sum_{i=1}^K \alpha_i U_i + X \beta$, where $U_i = 1$ if $Z = z_i$ and $0$ otherwise. Naturally, we would like to either include all of the variables $U_1, \ldots, U_K$ together in the model or drop them together. But the usual lasso has no such provision.

This problem is addressed by the group lasso method proposed by @yuan2006model. Suppose the covariate variables are partitioned in $G$ groups $X_1, \ldots, X_G$. Then, the group lasso loss function is
\begin{align*}
L_{group}(\beta_1, \ldots, \beta_G) = \frac{1}{2} \left\| Y - \sum_{g=1}^G X_g \beta_g \right\|^2_2 + \lambda \sum_{g=1}^G w_g \| \beta_g \|_2 .
\end{align*}
Here, $w_g$'s are weights assigned to the groups. One may take $w_g = 1$ for all $g$, which assigns equal weights to all the groups. However, one may also want to make larger groups less likely to be selected than a smaller group. The way to do that would be to pick a weighing scheme which assigns higher weights  to larger groups. In particular, the authors @yuan2006model recommended to take $w_g = s_g$, where $s_g$ is the size of the $g$th group, $g = 1, \ldots, G$, to address this problem. We shall take $w_g = 1$ for all $g$ for the sake of simplicity. As before, $\lambda \ge 0$ is a tuning parameter.

We minimize $L_{group}(\beta_1, \ldots, \beta_G)$ with respect to $\beta_1, \ldots, \beta_G$ to get the group lasso estimate. When all the group sizes are 1, the group lasso method coincides with the usual lasso method.

One drawback of the ususal group lasso is that it does not induce sparsity within the groups, i.e., if a group is selected in the model, all the coefficients within the group are usually non-zero. The casue of this is the $l_2$ norm within the groups, similar to the ridge regression. This problem can be mitigated using the underlying idea of the elastic net, and modifying the group lasso loss function to
\begin{align*}
L_{modgroup}(\beta_1, \ldots, \beta_G) = \frac{1}{2} \left\| Y - \sum_{g=1}^G X_g \beta_g \right\|^2_2 + \lambda \sum_{g=1}^G [ (1 - \alpha) \| \beta_g \|_2 + \alpha \| \beta_g \|_1 ] ,
\end{align*}
where $0 \le \alpha \le 1$. When $\alpha = 1$, the modified loss function $L_{modgroup}(\beta_1, \ldots, \beta_G)$ reduces to the usual lasso loss function, yielding the usual lasso estimate.

The tuning parameter $\lambda$ is selected based on cross validation. One may choose $\alpha$ either through cross validation, or may fix it beforehand.

### Fused lasso {#fusedlasso-description}

Sometimes, there is a spatial or temporal structure behind the data, and we want a degree of smoothness among the adjacent coefficents. Suppose the coefficients $\beta_i$ correspond to some quantity recorded on a one dimensional grid, and we do not want $\beta_{i+1}$ and $\beta_{i-1}$ to be much different from $\beta_i$. To achieve this goal, we may take the loss function as
\begin{align*}
L_{fused}(\beta) = \frac{1}{2} \| Y - X \beta \|^2_2 + \lambda_1 \| \beta \|_1 + \lambda_2 \sum_{i=2}^p | \beta_i - \beta_{i-1} | .
\end{align*}
Here, $\lambda_1 \ge 0, \lambda_2 \ge 0$ are tuning parameters. For positive $\lambda_2$, higher differences among consecutive coefficients are penalized higher. For spatial models, the coefficients may correspond to entities on a two or higher dimensional grid, and the concept of _adjacent_ coefficients is meaningful. To impose a degree of smoothness among the adjacent coefficients, we may use the loss function
\begin{align*}
L_{fusedgrid}(\beta) = \frac{1}{2} \| Y - X \beta \|^2_2 + \lambda_1 \| \beta \|_1 + \lambda_2 \sum_{i, j \in A} | \beta_i - \beta_j | ,
\end{align*}
where $A$ is the set of all pairs of adjacent indices. This way of penalization imposes a degree of _fusion_ among the adjacent coefficients (and hence the name fused lasso).

In some cases, we may not even have a regression problem, but the idea behind the fused lasso can still be applied. For example, suppose the response values are recorded on a one dimensional spatial or temporal grid, and we want to model the mean of the response variable. The mean is here a function over the grid, and we do not want much variation over adjacent grid points. To achieve that, we may take the loss function as
\begin{align*}
L_{simplefused}(\beta) = \frac{1}{2} \| Y - \beta \|^2_2 + \lambda_1 \| \beta \|_1 + \lambda_2 \sum_{i=2}^p | \beta_i - \beta_{i-1} | .
\end{align*}

The methodology of fused lasso was proposed by @tibshirani2005sparsity.

We have two tuning parameters here. Using careful arguments, some procedures for computing the fused lasso estimate reduce the problem of selecting two tuning parameters to the problem of selecting just one. Then, that one tuning parameter may be selected via cross validation. Other procedures also exist.

### Other regularizations {#otherregularizations-description}

Above, we have described several types of regularizations based on variations of the penalty term in the loss function. Through combinations of the underlying ideas behind them, many other forms of regularizations are possible. However, we have also change the first term of the loss function, which is fixed to be a squared error loss in all the formulations above. Observe that the squared error loss can be interpreted as the negative of the log likelihood of a Gaussian distribution (the associated constants would not matter in the minimization problem). So, based on this idea, we can readily construct other loss functions based on the negatives of the log likelihoods of other distributions.

We can also formulate such regularizations for generalized linear models, where, rather than modeling $E[Y]$ as a linear function of the covariate $X$, we model $g(E[Y]) = X \beta$.

In any such formulations, as long as the loss function is convex in $\beta$, solving the the minimization problem is not hard. However, some non-convex formulations also exist, but for general non-convex problems, getting the solution is computationally hard, and actually reaching the global minimum may not be guaranteed.

## Numerical demonstration {#regularizations-demonstration}

We shall use the `glmnet` package in R for the computations related to lasso, ridge regression, elastic net and adaptive lasso. The `glmnet` package is developed by @R-glmnet.

Let $\Sigma$ be a $100 \times 100$ covariance matrix defined by $\sigma_{i j} = 0.5 + 0.5 \mathbb{I}(i = j)$. Let $X$ be a $100$ dimensional random vector with $X \sim N_{100}( 0, \Sigma )$. Denote the $i$th coordinate of $X$ as $X_i$, $i = 1, \ldots, 100$. We consider the following regression problem: $Y = (1 - X_1 + 2 X_2 - 3 X_3 + 4 X_4 - 5 X_5 + 6 X_6) + e$, where $e$ is an error variable independent of $X$. We have a sample of size $30$ on $Y$ and $X$, based on which we shall estimate the coefficient vector using different types of regularizations.

```{r}

# Data generation; setting the seed for reproducible outcome
set.seed(1234)
Mu = rep(0, 100)
SIGMA = matrix(0.5, nrow = 100, ncol = 100) +
  diag(0.5, nrow = 100, ncol = 100)
X = MASS::mvrnorm(n = 30, mu = Mu, Sigma = SIGMA)
X_with_intercept = cbind(1, X)
Beta = c(c(1, -1, 2, -3, 4, -5, 6), rep(0, (100 - 6)))
Y = X_with_intercept %*% Beta + rnorm(30)

```

First we need to install the package `glmnet`.

```{r eval = FALSE}

install.packages('glmnet')

```

### Lasso computation {#lasso-computation}

We now demonstrate the lasso method on this model.

```{r message = FALSE}

# Loading the 'glmnet' package
require(glmnet)

# Constructing the lasso solution paths
fit_lasso = glmnet(X, Y)

```

We can plot the lasso solution paths using the `plot` function. The indices of the covariate variables are written on the left side. Notice that the coefficient paths for $X_6$ and $X_5$ are the most prominent, and then comes $X_4$. Do you understand the underlying cause?

```{r}

plot(fit_lasso, xvar = 'lambda', label = TRUE)

```

Using the function `cv.glmnet`, we can find the value of the cross-validated tuning parameter $\lambda$.

```{r}

cv_fit_lasso = cv.glmnet(X, Y)

```

We can visually inspect the mean-squared error for different values of lambda; the two bars along the mean-squared error curve are the upper and the lower standard deviation curves.

```{r}

plot(cv_fit_lasso)

```

In the plot above, we note that two specific $\lambda$ values are marked by vertical dotted lines. One of them, denoted by `lambda.min`, is the value of $\lambda$ which corresponds to the minimum cross-validated error. The other one, denoted by `lambda.1se`, is the largest value of $\lambda$ such that the corresponding cross-validated error is within one standard error of the minimum. This value of $\lambda$ corresponds to the _most regularized model_ with the cross-validated error being within one standard error of the minimum cross-validated error. In case of the lasso, since with increasing $\lambda$ the number of non-zero coefficients decreases, the number of non-zero coefficients corresponding to `lambda.1se` is typically lower than that corresponding to `lambda.min`.
 
We can access the lambda value corresponding to the minimum cross-validated error in the following way.

```{r}

lambda_min_lasso = cv_fit_lasso$lambda.min
writeLines(paste('Cross-validated lambda for lasso:',
                 lambda_min_lasso))

```

We can get the coefficients of the fitted model at the cross-validated lambda in the following way.

```{r}

coefficients_lasso = coef(cv_fit_lasso, s = 'lambda.min')
print(coefficients_lasso)

```

As can be seen above, the coefficients are returned in a sparse matrix format. We can convert that to the usual numeric format in the following way. Remember that the intercept term will occupy the first position in the resulting vector.

```{r}

coefficients_lasso = as.numeric(coefficients_lasso)

# Finding the number of zero coefficients
number_zero_coefficients_lasso = sum(coefficients_lasso == 0)
writeLines(paste('Number of zero coefficients in lasso for',
                 'cross-validated lambda:\n',
                 number_zero_coefficients_lasso))

# Finding which covariates have a non-zero coefficient in the
# fitted model; the '-1' accounts for the intercept term
indices_nonzero_coefficients_lasso =
  which(coefficients_lasso != 0) - 1

```

To get the coefficients at the value of $\lambda$ which gives the most regularized model such that the cross-validated error is within one standard error of the minimum, we use the following command.

```{r}

coefficients_lasso_most_regularized =
  coef(cv_fit_lasso, s = 'lambda.1se')
coefficients_lasso_most_regularized =
  as.numeric(coefficients_lasso_most_regularized)

# Finding the number of zero coefficients
number_zero_coefficients_lasso_most_regularized =
  sum(coefficients_lasso_most_regularized == 0)
writeLines(paste('Number of zero coefficients in lasso for \n',
                 'the most regularized model:',
                 number_zero_coefficients_lasso_most_regularized))

```

Note that the number of zero coefficients corresponding to `lambda.1se` is higher than that corresponding to `lambda.min`. We can find the coefficients at some other value of lambda also, in the same way.

```{r}

coefficients_lasso_s = coef(cv_fit_lasso, s = 0.5)
print(coefficients_lasso_s)

```

To predict the response at some value of the covariates, we use the following command. The argument `newx` must be supplied as a matrix and not a vector.

```{r}

x_0 = matrix(rnorm(100), nrow = 1, ncol = 100)
predicted_Y = predict(cv_fit_lasso, newx = x_0, s = 'lambda.min')
writeLines(paste('Predicted response:', predicted_Y))

```

We can predict at several sets of values of the covariates using the same command.

```{r}

x_0_values = matrix(rnorm(3*100), nrow = 3, ncol = 100)
predict(cv_fit_lasso, newx = x_0_values, s = 'lambda.min')

```

We can also predict at different values of lambda.

```{r}

x_0_values = matrix(rnorm(3*100), nrow = 3, ncol = 100)
predict(cv_fit_lasso, newx = x_0_values, s = c(0.5, 0.8))

```

### Ridge regression computation {#ridge-computation}

We next demonstrate the ridge regression method. The commands for the ridge regression method are identical with the lasso, except passing the value of another parameter `alpha = 0` to the `glmnet` function, which overwrites the default value of `alpha = 1`. The default value `alpha = 1` corresponds to the lasso method, and the value `alpha = 0` corresponds to ridge regression method. 

```{r}

# Constructing the ridge regression solution paths; 'alpha = 0'
# corresponds to the ridge regression method
fit_ridge = glmnet(X, Y, alpha = 0)

# Plotting the ridge regression solution paths
plot(fit_ridge, xvar = 'lambda', label = TRUE)

```

Note that the coefficients do not become zero with inreasing $\lambda$. The function 'cv.glmnet' with `alpha = 0` yields the value of the cross-validated $\lambda$ for the ridge regression.

```{r}

# 'alpha = 0' corresponds to the ridge regression
cv_fit_ridge = cv.glmnet(X, Y, alpha = 0)

# Plotting the mean-squared error for different values of lambda
plot(cv_fit_ridge)

# Accessing the lambda value corresponding to the minimum
# cross-validated error
lambda_min_ridge = cv_fit_ridge$lambda.min
writeLines(paste('Cross-validated lambda for ridge:',
                 lambda_min_ridge))

```

Below we obtain the coefficients of the fitted ridge regression model at the cross-validated lambda.

```{r}

coefficients_ridge = coef(cv_fit_ridge, s = 'lambda.min')
print(coefficients_ridge)

```

Note that the coefficients are nonzero unlike the lasso. In case of the ridge regression, the value `lambda.1se` is not so interesting as in case of the lasso, as here there is no question of reducing any coefficient to zero.

```{r}

coefficients_ridge = as.numeric(coefficients_ridge)

# Finding the number of nonzero coefficients; the '-1' accounts
# for the intercept term
number_nonzero_coefficients_ridge =
  sum(coefficients_ridge != 0) - 1
writeLines(paste('Number of nonzero coefficients for',
                 'the ridge regression:\n',
                 number_nonzero_coefficients_ridge))

```

Finding the coefficients at some other value of lambda is the same as in the case of the lasso.

```{r}

coefficients_ridge_s = coef(cv_fit_ridge, s = 0.5)
print(coefficients_ridge_s)

```

We can also predict at several sets of values of the covariates and at different values of lambda for the fitted ridge regression model. The argument `newx` always must be a matrix, even if we are predicting at only one set of values of the covariates.

```{r}

x_0_values = matrix(rnorm(3*100), nrow = 3, ncol = 100)
predict(cv_fit_ridge, newx = x_0_values, s = c(0.5, 0.8))

```

### Elastic net computation {#elasticnet-computation}

From the description of the elastic net in subsection \@ref(elasticnet-description), and the codes for computing the lasso and the ridge regression solutions in subsection \@ref(lasso-computation) and subsection \@ref(ridge-computation) respectively, you might have guessed that the function `glmnet` actually computed the elastic net solutions for different values of the parameter $\alpha$: $\alpha = 1$ corresponds to the lasso and $\alpha = 0$ corresponds to the ridge regression method. So, the same code with a value of the argument `alpha` in $(0, 1)$ will return the elastic net solution for that value of $\alpha$. Below, we demonstrate the computation for $\alpha = 0.5$.

```{r}

# Constructing the elastic net solution paths for 'alpha = 0.5'
fit_elnet = glmnet(X, Y, alpha = 0.5)

# Plotting the elastic net solution paths
plot(fit_elnet, xvar = 'lambda', label = TRUE)

```

Note that the coefficients become zero with inreasing $\lambda$ like in the case of the lasso.

```{r}

# Cross-validation for elastic net with 'alpha = 0.5'
cv_fit_elnet = cv.glmnet(X, Y, alpha = 0.5)

# Plotting the mean-squared error for different values of lambda
plot(cv_fit_elnet)

# Accessing the lambda value corresponding to the minimum
# cross-validated error
lambda_min_elnet = cv_fit_elnet$lambda.min
writeLines(paste('Cross-validated lambda for elastic net:',
                 lambda_min_elnet))

# Coefficients of the fitted elastic net at the cross-validated
# lambda
coefficients_elnet = coef(cv_fit_elnet, s = 'lambda.min')
print(coefficients_elnet)

```

Note that many of the coefficients are zero like the lasso. However, the number of zero coefficients is lower than the lasso.

```{r}

coefficients_elnet = as.numeric(coefficients_elnet)

# Finding the number of nonzero coefficients; the '-1' accounts
# for the intercept term
number_nonzero_coefficients_elnet =
  sum(coefficients_elnet != 0) - 1
writeLines(paste('Number of nonzero coefficients for',
                 'the elastic net:',
                 number_nonzero_coefficients_elnet))

# Computing the coefficients at some other value of lambda
coefficients_elnet_s = coef(cv_fit_elnet, s = 0.5)
print(coefficients_elnet_s)

# Predicting at several sets of values of the covariates and at
# different values of lambda for the fitted elastic net
x_0_values = matrix(rnorm(3*100), nrow = 3, ncol = 100)
predict(cv_fit_elnet, newx = x_0_values, s = c(0.5, 0.8))

```

### Adaptive lasso computation {#adaptivelasso-computation}

Recall the description of the adaptive lasso method in subsection \@ref(adaptivelasso-description). We shall demonstrate the adaptive lasso with $\gamma = 1$ taking the usual cross-validated lasso solution as the initial estimate.

First we compute the cross-validated lasso solution.

```{r}

# Computing the coefficient for the cross-validated lasso estimate
cv_fit_lasso = cv.glmnet(X, Y)
coefficients_lasso = coef(cv_fit_lasso, s = 'lambda.min')

# Dropping the intercept term in the cross-validated lasso
# estimate
coefficients_lasso = as.numeric(coefficients_lasso)[-1]

# Computing the adaptive lasso estimate. The argument
# 'penalty.factor' assigns weights to the coefficients, with the
# convention of r / 0 = Infinity for r > 0.
cv_fit_adaptive =
  cv.glmnet(X, Y, penalty.factor = 1 / abs(coefficients_lasso))
coefficients_adaptive = coef(cv_fit_adaptive, s = 'lambda.min')

# Dropping the intercept term for the adaptive lasso estimate
coefficients_adaptive = as.numeric(coefficients_adaptive)[-1]

```

Recall from subsection \@ref(adaptivelasso-description) that the adaptive lasso estimate is sparser than the lasso estimate. From the construction of our simulation setup, we know that only the coefficients of $X_1$, $X_2$, $X_3$, $X_4$, $X_5$ and $X_6$ are nonzero in the underlying model. Let us see the indices of the covariates with nonzero coefficients in the lasso estimate and the adaptive lasso estimate.

```{r}

writeLines(paste('Indices of the covariates with nonzero',
                 'coefficients \n for the lasso estimate:\n',
                 paste(which(coefficients_lasso != 0),
                       collapse = ' ')))

writeLines(paste('Indices of the covariates with nonzero',
                 'coefficients \n for the adaptive lasso',
                 'estimate:\n',
                 paste(which(coefficients_adaptive != 0),
                       collapse = ' ')))

```

We note that the lasso estimate has `r sum(coefficients_lasso != 0)` nonzero coordinates, while the adaptive lasso has `r sum(coefficients_adaptive != 0)` nonzero coordinates. So, the adaptive lasso estimate is indeed sparser than the lasso estimate. However, both of them erroneously estimate several zero coefficients as nonzero, and fail to capture several nonzero coefficients.

### Group lasso computation {#grouplasso-computation}

For the group lasso computation, we shall use the `gglasso` package by @R-gglasso. We shall use the same simulation setup, but the covariates will be divided in groups in the following way: $X_1, \ldots, X_5$ in the first group, $X_6, \ldots, X_10$ in the second group, and so on. We form the group indices below, which will be required.

```{r}

group_indices = c()
for (i in 1:20)
  group_indices = c(group_indices, rep(i, 5))
writeLines('First 20 group indices are:')
print(group_indices[1:20])

```

The following command installs the package `gglasso`.

```{r eval = FALSE}

install.packages('gglasso')

```

We now demonstrate the computation for the group lasso method. The commands are very similar to those in `glmnet`.

```{r message = FALSE}

# Loading the 'gglasso' package
require(gglasso)

# Constructing the group lasso solution paths
fit_group = gglasso(X, Y, group = group_indices)

# Plotting the ridge regression solution paths; the
# 'label' argument does not work here, unfortunately.
plot(fit_group, xvar = 'lambda')

# Cross-validation for group lasso; 'nfolds' is a
# cross-validation parameter whose default value
# was 10 in our earlier calculations using the 'glmnet'
# package. Since its default parameter for the 'gglasso'
# package is different, we set that explicitly to 10
# here to maintain uniformity.
cv_fit_group = cv.gglasso(X, Y, group = group_indices,
                          nfolds = 10)

# Plotting the mean-squared error for different values of
# lambda in the group lasso cross-validation
plot(cv_fit_group)

# Getting the lambda value corresponding to the minimum
# cross-validated error in the group lasso
lambda_min_group = cv_fit_group$lambda.min
writeLines(paste('Cross-validated lambda for group lasso:',
                 lambda_min_group))

# Coefficients of the fitted group lasso at the
# cross-validated lambda
coefficients_group = coef(cv_fit_group, s = 'lambda.min')
print(coefficients_group)

# Computing the coefficients at some other value of lambda
# for the group lasso estimate
coefficients_group_s = coef(cv_fit_group, s = 0.5)
print(coefficients_group_s)

```

Notice that the group lasso method reduces to zero all the coefficients together in any group. Next, we predict the group lasso estimate at some sets of values of the covariates.

```{r}

# Predicting at several sets of values of the covariates and at
# different values of lambda for the fitted group lasso; here
# 'type' is an argument whose value 'link' corresponds to
# regression prediction.
x_0_values = matrix(rnorm(3*100), nrow = 3, ncol = 100)
predict(cv_fit_group, newx = x_0_values, s = c(0.5, 0.8),
        type = 'link')

```

### Fused lasso computation {#fusedlasso-computation}

We shall demonstrate the idea of the fused lasso in a simpler model unlike the simulated regression setup used in the previous cases. Assume the response values are $Y_i$, where $i = 1, \ldots, n$, and $Y_i \sim \theta_i + e_i$, where $e_i$ are independent and identically distributed $N(0, 0.25)$ random variables. Suppose the sequence $\{\theta_i\}$ is piecewise constant, such that $\theta_1 = \ldots = \theta_{20} = 2$, $\theta_{21} = \ldots = \theta_{30} = 3$, $\theta_{31} = \ldots = \theta_{40} = 0$, $\theta_{41} = \ldots = \theta_{60} = 5$, $\theta_{61} = \ldots = \theta_{85} = 1$ and $\theta_{85} = \ldots = \theta_{100} = 0$. We want to find
$$\hat{\theta} = \arg\min_{\theta \in \mathbb{R}^n} \left[ \sum_{i=1}^n (Y_i - \theta_i)^2 + \lambda \sum_{i=1}^{n-1} | \theta_i - \theta_{i+1} | \right].$$
Below, we generate a sample from this model with $n = 100$.

```{r}

# Data generation for fused lasso with fixed seed
set.seed(1234)
Theta = c(rep(2, 20), rep(3, 10), rep(0, 10), rep(5, 20),
          rep(1, 25), rep(0, 15))
Y = Theta + rnorm(100, mean = 0, sd = sqrt(0.25))

```

We need the `genlasso` package [@R-genlasso] for the fused lasso computation. The following command would install this package.

```{r eval = FALSE}

install.packages('genlasso')

```

The fused lasso computation is demonstrated below.

```{r message = FALSE}

# Loading the 'genlasso' package
require(genlasso)

# Constructing the fused lasso solution paths; since
# the coefficients here are recorded on a one dimensional
# grid, we use the function 'fusedlasso1d'.
fit_fused = fusedlasso1d(Y)

# Plotting the fused lasso solution paths along with the
# actual observations; the points are the actual observations.
plot(fit_fused)

```

We note that the plot here is very different from the earlier examples, due to the difference in the nature of the problems (and also the packages used).

```{r echo = TRUE, results = 'hide'}

# Cross-validation for fused lasso; 'k' is  the
# cross-validation parameter corresponding to 'nfolds' in
# earlier cases. Cross-validation is done using the
# 'cv.trendfilter' function.
cv_fit_fused = cv.trendfilter(fit_fused, k = 10)

```

```{r}

# Plotting the mean-squared error for different values of
# lambda in the fused lasso cross-validation
plot(cv_fit_fused)

# Getting the lambda value corresponding to the minimum
# cross-validated error in the fused lasso
lambda_min_fused = cv_fit_fused$lambda.min
writeLines(paste('Cross-validated lambda for fused lasso:',
                 lambda_min_fused))

# Plotting the fused lasso estimate for the
# cross-validated lambda along with the actual observations
plot(fit_fused, lambda = lambda_min_fused,
     main = paste('Estimated coefficients for the',
                  'cross-validated lambda'))

```
