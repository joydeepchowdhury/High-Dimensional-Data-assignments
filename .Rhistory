p_BY = p.adjust(p, method = 'BY')
print(length(which(p <= 0.05)))
print(length(which(p_bonferroni <= 0.05)))
print(length(which(p_holm <= 0.05)))
print(length(which(p_hochberg <= 0.05)))
print(length(which(p_hommel <= 0.05)))
print(length(which(p_BH <= 0.05)))
print(length(which(p_BY <= 0.05)))
p = rbeta(1000, 1.5, 100)
# Calculating the adjusted p-values
p_bonferroni = p.adjust(p, method = 'bonferroni')
p_holm = p.adjust(p, method = 'holm')
p_hochberg = p.adjust(p, method = 'hochberg')
p_hommel = p.adjust(p, method = 'hommel')
p_BH = p.adjust(p, method = 'BH')
p_BY = p.adjust(p, method = 'BY')
print(length(which(p <= 0.05)))
print(length(which(p_bonferroni <= 0.05)))
print(length(which(p_holm <= 0.05)))
print(length(which(p_hochberg <= 0.05)))
print(length(which(p_hommel <= 0.05)))
print(length(which(p_BH <= 0.05)))
print(length(which(p_BY <= 0.05)))
p = rbeta(1000, 1.5, 100)
# Calculating the adjusted p-values
p_bonferroni = p.adjust(p, method = 'bonferroni')
p_holm = p.adjust(p, method = 'holm')
p_hochberg = p.adjust(p, method = 'hochberg')
p_hommel = p.adjust(p, method = 'hommel')
p_BH = p.adjust(p, method = 'BH')
p_BY = p.adjust(p, method = 'BY')
print(length(which(p <= 0.05)))
print(length(which(p_bonferroni <= 0.05)))
print(length(which(p_holm <= 0.05)))
print(length(which(p_hochberg <= 0.05)))
print(length(which(p_hommel <= 0.05)))
print(length(which(p_BH <= 0.05)))
print(length(which(p_BY <= 0.05)))
p = rbeta(1000, 1.5, 100)
# Calculating the adjusted p-values
p_bonferroni = p.adjust(p, method = 'bonferroni')
p_holm = p.adjust(p, method = 'holm')
p_hochberg = p.adjust(p, method = 'hochberg')
p_hommel = p.adjust(p, method = 'hommel')
p_BH = p.adjust(p, method = 'BH')
p_BY = p.adjust(p, method = 'BY')
print(length(which(p <= 0.05)))
print(length(which(p_bonferroni <= 0.05)))
print(length(which(p_holm <= 0.05)))
print(length(which(p_hochberg <= 0.05)))
print(length(which(p_hommel <= 0.05)))
print(length(which(p_BH <= 0.05)))
print(length(which(p_BY <= 0.05)))
p = rbeta(1000, 1.5, 150)
# Calculating the adjusted p-values
p_bonferroni = p.adjust(p, method = 'bonferroni')
p_holm = p.adjust(p, method = 'holm')
p_hochberg = p.adjust(p, method = 'hochberg')
p_hommel = p.adjust(p, method = 'hommel')
p_BH = p.adjust(p, method = 'BH')
p_BY = p.adjust(p, method = 'BY')
print(length(which(p <= 0.05)))
print(length(which(p_bonferroni <= 0.05)))
print(length(which(p_holm <= 0.05)))
print(length(which(p_hochberg <= 0.05)))
print(length(which(p_hommel <= 0.05)))
print(length(which(p_BH <= 0.05)))
print(length(which(p_BY <= 0.05)))
p = rbeta(1000, 1, 100)
# Calculating the adjusted p-values
p_bonferroni = p.adjust(p, method = 'bonferroni')
p_holm = p.adjust(p, method = 'holm')
p_hochberg = p.adjust(p, method = 'hochberg')
p_hommel = p.adjust(p, method = 'hommel')
p_BH = p.adjust(p, method = 'BH')
p_BY = p.adjust(p, method = 'BY')
print(length(which(p <= 0.05)))
print(length(which(p_bonferroni <= 0.05)))
print(length(which(p_holm <= 0.05)))
print(length(which(p_hochberg <= 0.05)))
print(length(which(p_hommel <= 0.05)))
print(length(which(p_BH <= 0.05)))
print(length(which(p_BY <= 0.05)))
p = rbeta(1000, 1, 100)
# Calculating the adjusted p-values
p_bonferroni = p.adjust(p, method = 'bonferroni')
p_holm = p.adjust(p, method = 'holm')
p_hochberg = p.adjust(p, method = 'hochberg')
p_hommel = p.adjust(p, method = 'hommel')
p_BH = p.adjust(p, method = 'BH')
p_BY = p.adjust(p, method = 'BY')
print(length(which(p <= 0.05)))
print(length(which(p_bonferroni <= 0.05)))
print(length(which(p_holm <= 0.05)))
print(length(which(p_hochberg <= 0.05)))
print(length(which(p_hommel <= 0.05)))
print(length(which(p_BH <= 0.05)))
print(length(which(p_BY <= 0.05)))
data('golub', package = 'multtest')
# Storing the data corresponding to ALL cases in the
# variable golub_ALL
golub_ALL = golub[,1:27]
# Storing the data corresponding to AML cases in the
# variable golub_AML
golub_AML = golub[,28:38]
# m is the number of tests, which is equal to the
# number of genes
m = nrow(golub)
# Computing the p-values using a two-sample t test
p = c()
for (i in 1:m)
p[i] = t.test(golub_ALL[i,], golub_AML[i,],
alternative = 'two.sided')$p.value
# p = rbeta(1000, 1, 100)
# Calculating the adjusted p-values
p_bonferroni = p.adjust(p, method = 'bonferroni')
p_holm = p.adjust(p, method = 'holm')
p_hochberg = p.adjust(p, method = 'hochberg')
p_hommel = p.adjust(p, method = 'hommel')
p_BH = p.adjust(p, method = 'BH')
p_BY = p.adjust(p, method = 'BY')
print(length(which(p <= 0.05)))
print(length(which(p_bonferroni <= 0.05)))
print(length(which(p_holm <= 0.05)))
print(length(which(p_hochberg <= 0.05)))
print(length(which(p_hommel <= 0.05)))
print(length(which(p_BH <= 0.05)))
print(length(which(p_BY <= 0.05)))
data('golub', package = 'multtest')
# Storing the data corresponding to ALL cases in the
# variable golub_ALL
golub_ALL = golub[,1:27]
# Storing the data corresponding to AML cases in the
# variable golub_AML
golub_AML = golub[,28:38]
# m is the number of tests, which is equal to the
# number of genes
m = nrow(golub)
# Computing the p-values using a two-sample t test
p = c()
for (i in 1:m)
p[i] = t.test(golub_ALL[i,], golub_AML[i,],
alternative = 'two.sided')$p.value
# p = rbeta(1000, 1, 100)
# Calculating the adjusted p-values
p_bonferroni = p.adjust(p, method = 'bonferroni')
p_holm = p.adjust(p, method = 'holm')
p_hochberg = p.adjust(p, method = 'hochberg')
p_hommel = p.adjust(p, method = 'hommel')
p_BH = p.adjust(p, method = 'BH')
p_BY = p.adjust(p, method = 'BY')
print(length(which(p <= 0.05)))
print(length(which(p_bonferroni <= 0.05)))
print(length(which(p_holm <= 0.05)))
print(length(which(p_hochberg <= 0.05)))
print(length(which(p_hommel <= 0.05)))
print(length(which(p_BH <= 0.05)))
print(length(which(p_BY <= 0.05)))
SIGMA = matrix(0.5, nrow = 10, ncol = 10) + diag(0.5, nrow = 10, ncol = 10)
SIGMA
install.packages('glmnet')
# Data generation; setting the seed for reproducible outcome
set.seed(1234)
Mu = rep(0, 100)
SIGMA = matrix(0.5, nrow = 100, ncol = 100) + diag(0.5, nrow = 100, ncol = 100)
X = MASS::mvrnorm(n = 30, mu = Mu, Sigma = SIGMA)
X_with_intercept = cbind(1, X)
Beta = c(c(1, -1, 2, -3, 4, -5, 6), rep(0, (100 - 6)))
Y = X_with_intercept %*% t(Beta) + rnorm(30)
a = t(Beta)
# Data generation; setting the seed for reproducible outcome
set.seed(1234)
Mu = rep(0, 100)
SIGMA = matrix(0.5, nrow = 100, ncol = 100) + diag(0.5, nrow = 100, ncol = 100)
X = MASS::mvrnorm(n = 30, mu = Mu, Sigma = SIGMA)
X_with_intercept = cbind(1, X)
Beta = c(c(1, -1, 2, -3, 4, -5, 6), rep(0, (100 - 6)))
Y = X_with_intercept %*% Beta + rnorm(30)
# Loading the 'glmnet' package
require(glmnet)
# Constructing the lasso solution path
fit_lasso = glmnet(X, Y)
# Plotting the lasso solution path
plot(fit_lasso)
# Using the function 'cv.glmnet', we can find the value of the
# cross-validated tuning parameter Lambda
cv_fit_lasso = cv.glmnet(X, Y)
# We can visually inspect the mean-squared error for different
# values of Lambda; the two bars along the mean-squared error
# curve are the upper and the lower standard deviation curves.
plot(cv_fit_lasso)
# We can access the Lambda value corresponding to the minimum
# cross-validated error in the following way
lambda_min_lasso = cv_fit_lasso$lambda.min
writeLines(paste('Cross-validated Lambda for lasso:',
lambda_min_lasso))
exp(-1)
# We can get the coefficients of the fitted model at the
# cross-validated Lambda in the following way
coefficients_lasso = coef(cv_fit_lasso, s = 'lambda.min')
coefficients_lasso
as.numeric(coefficients_lasso)
which(as.numeric(coefficients_lasso) != 0)
print(coefficients_lasso)
# Data generation; setting the seed for reproducible outcome
set.seed(1234)
Mu = rep(0, 100)
SIGMA = matrix(0.5, nrow = 100, ncol = 100) + diag(0.5, nrow = 100, ncol = 100)
X = MASS::mvrnorm(n = 30, mu = Mu, Sigma = SIGMA)
X_with_intercept = cbind(1, X)
Beta = c(c(1, -1, 2, -3, 4, -5, 6), rep(0, (100 - 6)))
Y = X_with_intercept %*% Beta + rnorm(30)
# Loading the 'glmnet' package
require(glmnet)
# Constructing the lasso solution path
fit_lasso = glmnet(X, Y)
# Plotting the lasso solution path
plot(fit_lasso)
# Using the function 'cv.glmnet', we can find the value of the
# cross-validated tuning parameter lambda.
cv_fit_lasso = cv.glmnet(X, Y)
# We can visually inspect the mean-squared error for different
# values of lambda; the two bars along the mean-squared error
# curve are the upper and the lower standard deviation curves.
plot(cv_fit_lasso)
# We can access the lambda value corresponding to the minimum
# cross-validated error in the following way.
lambda_min_lasso = cv_fit_lasso$lambda.min
writeLines(paste('Cross-validated lambda for lasso:',
lambda_min_lasso))
# We can get the coefficients of the fitted model at the
# cross-validated lambda in the following way.
coefficients_lasso = coef(cv_fit_lasso, s = 'lambda.min')
print(coefficients_lasso)
# As can be seen above, the coefficients are returned in a
# sparse matrix format. We can convert that to the usual
# numeric format in the following way. Remember that the
# intercept term will occupy the first position in the
# resulting vector!
coefficients_lasso = as.numeric(coefficients_lasso)
# Finding which coordinates of the covariate vector has a
# non-zero coefficient in the fitted model; the '-1' accounts
# for the intercept term
indices_nonzero_coefficients_lasso =
which(coefficients_lasso != 0) - 1
# We can find the coefficients at some other value of lambda
# also, in the same way.
coefficients_lasso_s = coef(cv_fit_lasso, s = 0.5)
print(coefficients_lasso_s)
# To predict the response at some value of the covariate,
# we use the following command.
x_0 = rnorm(100)
predicted_Y = predict(cv_fit_lasso, newx = x_0, s = 'lambda.min')
# Constructing the ridge regression solution path; 'alpha = 0'
# corresponds to the ridge regression method
fit_ridge = glmnet(X, Y, alpha = 0)
require(glmnet)
source('~/.active-rstudio-document')
# Data generation; setting the seed for reproducible outcome
set.seed(1234)
Mu = rep(0, 100)
SIGMA = matrix(0.5, nrow = 100, ncol = 100) +
diag(0.5, nrow = 100, ncol = 100)
X = MASS::mvrnorm(n = 30, mu = Mu, Sigma = SIGMA)
X_with_intercept = cbind(1, X)
Beta = c(c(1, -1, 2, -3, 4, -5, 6), rep(0, (100 - 6)))
Y = X_with_intercept %*% Beta + rnorm(30)
# Constructing the ridge regression solution path; 'alpha = 0'
# corresponds to the ridge regression method
fit_ridge = glmnet(X, Y, alpha = 0)
plot(fit_ridge, xvar = 'lambda', label = TRUE)
# 'alpha = 0' corresponds to the ridge regression
cv_fit_ridge = cv.glmnet(X, Y, alpha = 0)
plot(cv_fit_ridge)
cv_fit_ridge$cvm
cv_fit_ridge$glmnet.fit$lambda
# Data generation; setting the seed for reproducible outcome
set.seed(1234)
Mu = rep(0, 100)
SIGMA = matrix(0.5, nrow = 100, ncol = 100) +
diag(0.5, nrow = 100, ncol = 100)
X = MASS::mvrnorm(n = 30, mu = Mu, Sigma = SIGMA)
X_with_intercept = cbind(1, X)
Beta = c(c(1, -1, 2, -3, 4, -5, 6), rep(0, (100 - 6)))
Y = X_with_intercept %*% Beta + rnorm(30)
# Constructing the ridge regression solution path; 'alpha = 0'
# corresponds to the ridge regression method
fit_ridge = glmnet(X, Y, alpha = 0)
plot(fit_ridge, xvar = 'lambda', label = TRUE)
# 'alpha = 0' corresponds to the ridge regression
cv_fit_ridge = cv.glmnet(X, Y, alpha = 0)
plot(cv_fit_ridge)
lambda_min_ridge = cv_fit_ridge$lambda.min
cv_fit_ridge$nzero
cv_fit_ridge$lambda
cv_fit_ridge$cvm
which.min(cv_fit_ridge$cvm)
which(cv_fit_ridge$lambda == cv_fit_ridge$lambda.min)
# Data generation; setting the seed for reproducible outcome
set.seed(1234)
Mu = rep(0, 100)
SIGMA = matrix(0.5, nrow = 100, ncol = 100) +
diag(0.5, nrow = 100, ncol = 100)
X = MASS::mvrnorm(n = 30, mu = Mu, Sigma = SIGMA)
X_with_intercept = cbind(1, X)
Beta = c(c(1, -1, 2, -3, 4, -5, 6), rep(0, (100 - 6)))
Y = X_with_intercept %*% Beta + rnorm(30)
# Computing the coefficient for the cross-validated lasso
cv_fit_lasso = cv.glmnet(X, Y)
lambda_min_lasso = cv_fit_lasso$lambda.min
coefficients_lasso = coef(cv_fit_lasso, s = 'lambda.min')
coefficients_lasso
coefficients_lasso = as.numeric(coefficients_lasso)
coefficients_lasso
# Dropping the intercept term
coefficients_lasso = coefficients_lasso[-1]
coefficients_lasso
# Data generation; setting the seed for reproducible outcome
set.seed(1234)
Mu = rep(0, 100)
SIGMA = matrix(0.5, nrow = 100, ncol = 100) +
diag(0.5, nrow = 100, ncol = 100)
X = MASS::mvrnorm(n = 30, mu = Mu, Sigma = SIGMA)
X_with_intercept = cbind(1, X)
Beta = c(c(1, -1, 2, -3, 4, -5, 6), rep(0, (100 - 6)))
Y = X_with_intercept %*% Beta + rnorm(30)
# Computing the coefficient for the cross-validated lasso
cv_fit_lasso = cv.glmnet(X, Y)
coefficients_lasso = coef(cv_fit_lasso, s = 'lambda.min')
coefficients_lasso = as.numeric(coefficients_lasso)
# Dropping the intercept term
coefficients_lasso = coefficients_lasso[-1]
# Dropping the covariates with zero coefficients
X_adaptive = X[, -which(coefficients_lasso == 0)]
# Storing the indices of the covariates chosen
Covariate_indices_chosen =
(1:ncol(X))[-which(coefficients_lasso == 0)]
# Computing the weights
weights_adaptive =
1 / abs(coefficients_lasso[-which(coefficients_lasso == 0)])
# The argument 'weights' assigns weights to the coefficients.
cv_fit_adaptive = cv.glmnet(X_adaptive, Y,
weights = weights_adaptive)
# Computing the coefficients for the cross-validated model
coefficients_adaptive = coef(cv_fit_adaptive, s = 'lambda.min')
coefficients_adaptive = as.numeric(coefficients_adaptive)
# Data generation; setting the seed for reproducible outcome
set.seed(1234)
Mu = rep(0, 100)
SIGMA = matrix(0.5, nrow = 100, ncol = 100) +
diag(0.5, nrow = 100, ncol = 100)
X = MASS::mvrnorm(n = 30, mu = Mu, Sigma = SIGMA)
X_with_intercept = cbind(1, X)
Beta = c(c(1, -1, 2, -3, 4, -5, 6), rep(0, (100 - 6)))
Y = X_with_intercept %*% Beta + rnorm(30)
# Computing the coefficient for the cross-validated lasso
cv_fit_lasso = cv.glmnet(X, Y)
coefficients_lasso = coef(cv_fit_lasso, s = 'lambda.min')
coefficients_lasso = as.numeric(coefficients_lasso)
# Dropping the intercept term
coefficients_lasso = coefficients_lasso[-1]
# Dropping the covariates with zero coefficients
X_adaptive = X[, -which(coefficients_lasso == 0)]
# Storing the indices of the covariates chosen
Covariate_indices_chosen =
(1:ncol(X))[-which(coefficients_lasso == 0)]
# Computing the weights
weights_adaptive =
1 / abs(coefficients_lasso[-which(coefficients_lasso == 0)])
# The argument 'penalty.factor' assigns weights to the coefficients.
cv_fit_adaptive = cv.glmnet(X_adaptive, Y,
penalty.factor = weights_adaptive)
# Computing the coefficients for the cross-validated model
coefficients_adaptive = coef(cv_fit_adaptive, s = 'lambda.min')
coefficients_adaptive = as.numeric(coefficients_adaptive)
coefficients_adaptive = coefficients_adaptive[-1]
Covariate_indices_chosen[coefficients_adaptive != 0]
Covariate_indices_chosen
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
weights_adaptive
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
1 / abs(coefficients_lasso)
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
require(gglasso)
install.packages("gglasso")
require(gglasso)
# Data generation; setting the seed for reproducible outcome
set.seed(1234)
Mu = rep(0, 100)
SIGMA = matrix(0.5, nrow = 100, ncol = 100) +
diag(0.5, nrow = 100, ncol = 100)
X = MASS::mvrnorm(n = 30, mu = Mu, Sigma = SIGMA)
X_with_intercept = cbind(1, X)
Beta = c(c(1, -1, 2, -3, 4, -5, 6), rep(0, (100 - 6)))
Y = X_with_intercept %*% Beta + rnorm(30)
group_indices = c()
for (i in 1:20)
group_indices = c(group_indices, rep(i, 5))
writeLines('First 20 group indices are:')
print(group_indices[1:20])
install.packages('gglasso')
install.packages("gglasso")
# Loading the 'glmnet' package
require(gglasso)
# Constructing the group lasso solution path
fit_group = gglasso(X, Y, group = group_indices)
plot(fit_group, xvar = 'lambda', label = TRUE)
a = fit_group$beta
View(a)
source('~/.active-rstudio-document')
install.packages("gglasso")
a = fit_group$beta
View(a)
install.packages('genlasso')
require(genlasso)
fit_fused = fusedlasso1d(Y)
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
plot(cv_fit_fused, lambda = lambda_min_fused)
plot(fit_fused, lambda = lambda_min_fused)
source('~/.active-rstudio-document')
points(Y, col = 'blue')
# Data generation for fused lasso with fixed seed
set.seed(1234)
Theta = c(rep(2, 20), rep(3, 10), rep(0, 10), rep(5, 20), rep(1, 25),
rep(0, 15))
Y = Theta + rnorm(100)
plot(Y)
# Data generation for fused lasso with fixed seed
set.seed(1234)
Theta = c(rep(2, 20), rep(3, 10), rep(0, 10), rep(5, 20), rep(1, 25),
rep(0, 15))
Y = Theta + rnorm(100, 0, 0.01)
plot(Y)
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
# Data generation for fused lasso with fixed seed
set.seed(1234)
Theta = c(rep(2, 20), rep(3, 10), rep(0, 10), rep(5, 20), rep(1, 25),
rep(0, 15))
Y = Theta + rnorm(100, 0, sqrt(0.25))
plot(Y)
# Loading the 'genlasso' package
require(genlasso)
# Constructing the fused lasso solution paths; since
# the coefficients here are recorded on a one dimensional
# grid, we use the function 'fusedlasso1d'.
fit_fused = fusedlasso1d(Y)
plot(fit_fused)
# Cross-validation for fused lasso; 'k' is  the
# cross-validation parameter corresponding to 'nfolds' in
# earlier cases. Cross-validation is done using the
# 'cv.trendfilter' function. The 'suppressMessages'
# function suppresses unneeded messages
cv_fit_fused = suppressMessages(cv.trendfilter(fit_fused, k = 10))
cv.trendfilter(fit_fused, k = 10)
cv.trendfilter(fit_fused, k = 10, verbose = FALSE)
source('~/.active-rstudio-document')
genlasso::cv.trendfilter()
genlasso::cv.trendfilter
# Data generation for fused lasso with fixed seed
set.seed(1234)
Theta = c(rep(2, 20), rep(3, 10), rep(0, 10), rep(5, 20), rep(1, 25),
rep(0, 15))
Y = Theta + rnorm(100, 0, sqrt(0.25))
plot(Y)
# Loading the 'genlasso' package
require(genlasso)
# Constructing the fused lasso solution paths; since
# the coefficients here are recorded on a one dimensional
# grid, we use the function 'fusedlasso1d'.
fit_fused = fusedlasso1d(Y)
# Plotting the fused lasso solution paths along with the
# actual observations; the points are the actual observations.
plot(fit_fused)
# Cross-validation for fused lasso; 'k' is  the
# cross-validation parameter corresponding to 'nfolds' in
# earlier cases. Cross-validation is done using the
# 'cv.trendfilter' function. The 'suppressMessages'
# function suppresses unneeded messages
# invisible(capture.output(cv_fit_fused = cv.trendfilter(fit_fused, k = 10)))
sink(tempfile())
cv_fit_fused = cv.trendfilter(fit_fused, k = 10)
sink()
source('~/.active-rstudio-document')
.packages()
source('~/.active-rstudio-document')
file.create('.nojekyll')
install.packages("rmarkdown")
install.packages("bookdown")
install.packages("glmnet")
install.packages("gglasso")
install.packages("multtest")
BiocManager::available()
BiocManager::install('multtest')
